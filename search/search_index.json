{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Aviato SDK Documentation","text":"<p>Python client library for Aviato sandboxes - a remote code execution platform.</p>"},{"location":"#installation","title":"Installation","text":"<p>Clone and install the SDK locally:</p> <pre><code>git clone https://github.com/coreweave/aviato-client.git\ncd aviato-client\nuv sync\n</code></pre>"},{"location":"#authentication","title":"Authentication","text":"<p>The SDK checks for credentials in this order:</p>"},{"location":"#option-1-aviato-api-key-recommended","title":"Option 1: Aviato API Key (Recommended)","text":"<pre><code>export AVIATO_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"#option-2-wb-credentials","title":"Option 2: W&amp;B Credentials","text":"<p>If you have W&amp;B credentials configured, the SDK can use them:</p> <pre><code>export WANDB_API_KEY=\"your-wandb-key\"\nexport WANDB_ENTITY_NAME=\"your-entity\"\n</code></pre> <p>The SDK also reads W&amp;B credentials from <code>~/.netrc</code> if <code>WANDB_API_KEY</code> isn't set:</p> <pre><code>machine api.wandb.ai\n  login user\n  password your-wandb-key\n</code></pre> <p><code>WANDB_ENTITY_NAME</code> is still required when using netrc.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from aviato import Sandbox\n\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"echo\", \"Hello, Aviato!\"]).result()\n    print(result.stdout)  # \"Hello, Aviato!\\n\"\n</code></pre>"},{"location":"#core-concepts","title":"Core Concepts","text":""},{"location":"#non-blocking-by-default","title":"Non-Blocking by Default","text":"<p>Operations return immediately. Blocking happens explicitly when you need results:</p> Operation Returns Block with <code>Sandbox.run()</code> <code>Sandbox</code> <code>.wait()</code> <code>sandbox.exec()</code> <code>Process</code> <code>.result()</code> <code>sandbox.read_file()</code> <code>OperationRef</code> <code>.result()</code> <code>sandbox.write_file()</code> <code>OperationRef</code> <code>.result()</code> <code>sandbox.stop()</code> <code>OperationRef</code> <code>.result()</code> <p>This enables natural parallelism - start multiple operations, then collect results:</p> <pre><code>refs = [sandbox.read_file(f\"/app/file{i}.txt\") for i in range(10)]\ncontents = [ref.result() for ref in refs]\n</code></pre>"},{"location":"#sandbox-lifecycle","title":"Sandbox Lifecycle","text":"<p>Sandboxes progress through these states:</p> <pre><code>PENDING -&gt; CREATING -&gt; RUNNING -&gt; (COMPLETED | FAILED | TERMINATED)\n</code></pre> <p>Most operations handle state transitions automatically. For example, <code>exec()</code> waits for RUNNING before executing.</p>"},{"location":"#tutorial","title":"Tutorial","text":"<p>New to Aviato? The Tutorial walks you through the SDK step by step, from creating your first sandbox to cleanup patterns.</p>"},{"location":"#guides","title":"Guides","text":"Guide Topic Execution Commands, streaming, timeouts, working directories File Operations Reading, writing, parallel transfers Sessions Managing multiple sandboxes with shared config Remote Functions The <code>@session.function()</code> decorator Sandbox Configuration Resources, mounted files, ports Cleanup Patterns Resource management, orphan handling Sync vs Async When to use sync vs async patterns Troubleshooting Common issues and solutions"},{"location":"#examples","title":"Examples","text":"<p>See the examples directory for runnable Python scripts.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>For class internals, authentication flow, and implementation details, see AGENTS.md.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#aviato","title":"aviato","text":"<p>A Python client library for Aviato sandboxes.</p>"},{"location":"api/#aviato.SandboxDefaults","title":"SandboxDefaults  <code>dataclass</code>","text":"<pre><code>SandboxDefaults(\n    container_image: str = DEFAULT_CONTAINER_IMAGE,\n    command: str = DEFAULT_COMMAND,\n    args: tuple[str, ...] = DEFAULT_ARGS,\n    base_url: str = DEFAULT_BASE_URL,\n    request_timeout_seconds: float = DEFAULT_REQUEST_TIMEOUT_SECONDS,\n    max_lifetime_seconds: float\n    | None = DEFAULT_MAX_LIFETIME_SECONDS,\n    temp_dir: str = DEFAULT_TEMP_DIR,\n    tags: tuple[str, ...] = tuple(),\n    runway_ids: tuple[str, ...] | None = None,\n    tower_ids: tuple[str, ...] | None = None,\n    resources: dict[str, Any] | None = None,\n    environment_variables: dict[str, str] = dict(),\n)\n</code></pre> <p>Immutable configuration defaults for sandbox creation.</p> <p>All fields have sensible defaults. Override only what you need.</p> <p>There are two separate timeout concepts: - request_timeout_seconds: How long to wait for API responses (client-side) - max_lifetime_seconds: How long the sandbox runs before auto-termination (server-side)   If not set, the backend controls the default lifetime.</p> <p>Tags enable filtering and organizing sandboxes. They are propagated to the backend and can be used to query sandboxes by tag.</p> Example <pre><code>defaults = SandboxDefaults(\n    container_image=\"python:3.12\",\n    command=\"tail\",\n    args=(\"-f\", \"/dev/null\"),\n    request_timeout_seconds=60,\n    max_lifetime_seconds=3600,  # 1 hour sandbox lifetime\n    tags=(\"my-workload\", \"experiment-42\"),\n    environment_variables={\"LOG_LEVEL\": \"info\", \"REGION\": \"us-west\"},\n)\n</code></pre>"},{"location":"api/#aviato.SandboxDefaults.merge_tags","title":"merge_tags","text":"<pre><code>merge_tags(additional: list[str] | None) -&gt; list[str]\n</code></pre> <p>Combine default tags with additional tags.</p> <p>Tags from both sources are included. Order is: defaults first, then additional tags appended.</p>"},{"location":"api/#aviato.SandboxDefaults.merge_environment_variables","title":"merge_environment_variables","text":"<pre><code>merge_environment_variables(\n    additional: dict[str, str] | None,\n) -&gt; dict[str, str]\n</code></pre> <p>Combine default environment variables with additional ones.</p> <p>Additional environment variables override defaults with the same key.</p>"},{"location":"api/#aviato.SandboxDefaults.with_overrides","title":"with_overrides","text":"<pre><code>with_overrides(**kwargs: Any) -&gt; SandboxDefaults\n</code></pre> <p>Create new defaults with some values overridden.</p>"},{"location":"api/#aviato.Sandbox","title":"Sandbox","text":"<pre><code>Sandbox(\n    *,\n    command: str | None = None,\n    args: list[str] | None = None,\n    defaults: SandboxDefaults | None = None,\n    container_image: str | None = None,\n    tags: list[str] | None = None,\n    base_url: str | None = None,\n    request_timeout_seconds: float | None = None,\n    max_lifetime_seconds: float | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    resources: dict[str, Any] | None = None,\n    mounted_files: list[dict[str, Any]] | None = None,\n    s3_mount: dict[str, Any] | None = None,\n    ports: list[dict[str, Any]] | None = None,\n    service: dict[str, Any] | None = None,\n    max_timeout_seconds: int | None = None,\n    environment_variables: dict[str, str] | None = None,\n    _session: Session | None = None,\n)\n</code></pre> <p>Aviato sandbox client with sync/async hybrid API.</p> <p>All methods return immediately and can be used in both sync and async contexts. Operations are executed in a background event loop managed by _LoopManager.</p> <p>Examples:</p> <p>Factory method: <pre><code>sb = Sandbox.run(\"echo\", \"hello\")  # Returns immediately\nresult = sb.exec([\"echo\", \"more\"]).result()  # Block for result\nsb.stop().result()  # Block for completion\n</code></pre></p> <p>Context manager (recommended): <pre><code>with Sandbox.run(\"sleep\", \"infinity\") as sb:\n    result = sb.exec([\"echo\", \"hello\"]).result()\n# Automatically stopped on exit\n</code></pre></p> <p>Async context manager: <pre><code>async with Sandbox.run(\"sleep\", \"infinity\") as sb:\n    result = await sb.exec([\"echo\", \"hello\"])\n</code></pre></p> ATTRIBUTE DESCRIPTION <code>sandbox_id</code> <p>Unique identifier for this sandbox.</p> <p> TYPE: <code>str | None</code> </p> <code>status</code> <p>Cached status from last API call.</p> <p> TYPE: <code>SandboxStatus | None</code> </p> <code>tower_id</code> <p>Tower ID where sandbox is running.</p> <p> TYPE: <code>str | None</code> </p> <code>runway_id</code> <p>Runway ID for this sandbox.</p> <p> TYPE: <code>str | None</code> </p> <code>returncode</code> <p>Exit code if sandbox completed.</p> <p> TYPE: <code>int | None</code> </p> <code>started_at</code> <p>When sandbox started running.</p> <p> TYPE: <code>datetime | None</code> </p> <p>Initialize a sandbox (does not start it).</p> PARAMETER DESCRIPTION <code>command</code> <p>Optional command to run in the sandbox</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>args</code> <p>Optional arguments for the command</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>defaults</code> <p>Optional SandboxDefaults to apply</p> <p> TYPE: <code>SandboxDefaults | None</code> DEFAULT: <code>None</code> </p> <code>container_image</code> <p>Container image to use (default: python:3.11)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Optional tags for the sandbox</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>base_url</code> <p>Aviato API URL (default: AVIATO_BASE_URL env or localhost)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>request_timeout_seconds</code> <p>Timeout for API requests (client-side, default: 300s)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>max_lifetime_seconds</code> <p>Max sandbox lifetime (server-side). If not set, the backend controls the default.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Optional list of runway IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Optional list of tower IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resource requests (CPU, memory, GPU)</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>mounted_files</code> <p>Files to mount into the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>s3_mount</code> <p>S3 bucket mount configuration</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ports</code> <p>Port mappings for the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>service</code> <p>Service configuration for network access</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>max_timeout_seconds</code> <p>Maximum timeout for sandbox operations</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>environment_variables</code> <p>Environment variables to inject into the sandbox. Merges with and overrides matching keys from the session defaults. Use for non-sensitive config only.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/#aviato.Sandbox.sandbox_id","title":"sandbox_id  <code>property</code>","text":"<pre><code>sandbox_id: str | None\n</code></pre> <p>The unique sandbox ID, or None if not yet started.</p>"},{"location":"api/#aviato.Sandbox.returncode","title":"returncode  <code>property</code>","text":"<pre><code>returncode: int | None\n</code></pre> <p>Exit code if sandbox has completed, None if still running.</p> <p>Use wait() to block until the sandbox completes.</p>"},{"location":"api/#aviato.Sandbox.tower_id","title":"tower_id  <code>property</code>","text":"<pre><code>tower_id: str | None\n</code></pre> <p>Tower where sandbox is running, or None if not started.</p>"},{"location":"api/#aviato.Sandbox.runway_id","title":"runway_id  <code>property</code>","text":"<pre><code>runway_id: str | None\n</code></pre> <p>Runway where sandbox is running, or None if not started.</p>"},{"location":"api/#aviato.Sandbox.status","title":"status  <code>property</code>","text":"<pre><code>status: SandboxStatus | None\n</code></pre> <p>Last known status of the sandbox.</p> <p>This is the cached status from the most recent API interaction.</p> <p>Returns None only for sandboxes that haven't been started yet.</p> <p>Note: This value may be stale. Check status_updated_at for when it was last fetched. For guaranteed fresh status, use <code>await sandbox.get_status()</code> which always hits the API.</p>"},{"location":"api/#aviato.Sandbox.status_updated_at","title":"status_updated_at  <code>property</code>","text":"<pre><code>status_updated_at: datetime | None\n</code></pre> <p>Timestamp when status was last fetched from the API.</p> <p>Returns None only for sandboxes that haven't been started yet.</p>"},{"location":"api/#aviato.Sandbox.started_at","title":"started_at  <code>property</code>","text":"<pre><code>started_at: datetime | None\n</code></pre> <p>Timestamp when the sandbox was started.</p> <p>Populated after start() completes or when obtained via list()/from_id(). None only for sandboxes that haven't been started yet.</p>"},{"location":"api/#aviato.Sandbox.tower_group_id","title":"tower_group_id  <code>property</code>","text":"<pre><code>tower_group_id: str | None\n</code></pre> <p>Tower group ID where the sandbox is running.</p>"},{"location":"api/#aviato.Sandbox.service_address","title":"service_address  <code>property</code>","text":"<pre><code>service_address: str | None\n</code></pre> <p>External address for accessing sandbox services.</p> <p>Returns an address like '166.19.9.70:8080' for network-accessible sandboxes (SSH, web services). Availability depends on tower configuration.</p> <p>Returns None if: - Sandbox hasn't been started yet - Sandbox was obtained via from_id() or list() - Tower uses ClusterIP instead of LoadBalancer</p>"},{"location":"api/#aviato.Sandbox.exposed_ports","title":"exposed_ports  <code>property</code>","text":"<pre><code>exposed_ports: tuple[tuple[int, str], ...] | None\n</code></pre> <p>Exposed ports for the sandbox.</p> <p>Returns a tuple of (container_port, name) tuples for ports exposed by the sandbox. Useful for network-accessible sandboxes.</p> <p>Returns None if: - Sandbox hasn't been started yet - Sandbox was obtained via from_id() or list() - No ports were exposed</p>"},{"location":"api/#aviato.Sandbox.execution_stats","title":"execution_stats  <code>property</code>","text":"<pre><code>execution_stats: dict[str, int]\n</code></pre> <p>Execution statistics for this sandbox.</p> RETURNS DESCRIPTION <code>dict[str, int]</code> <p>Dictionary with execution counts:</p> <code>dict[str, int]</code> <ul> <li>total: Total number of exec() calls</li> </ul> <code>dict[str, int]</code> <ul> <li>successes: Commands with returncode=0</li> </ul> <code>dict[str, int]</code> <ul> <li>failures: Commands with returncode!=0</li> </ul> <code>dict[str, int]</code> <ul> <li>errors: Commands that errored (timeout, cancellation, etc.)</li> </ul>"},{"location":"api/#aviato.Sandbox.run","title":"run  <code>classmethod</code>","text":"<pre><code>run(\n    *args: str,\n    container_image: str | None = None,\n    defaults: SandboxDefaults | None = None,\n    request_timeout_seconds: float | None = None,\n    max_lifetime_seconds: float | None = None,\n    tags: list[str] | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    resources: dict[str, Any] | None = None,\n    mounted_files: list[dict[str, Any]] | None = None,\n    s3_mount: dict[str, Any] | None = None,\n    ports: list[dict[str, Any]] | None = None,\n    service: dict[str, Any] | None = None,\n    max_timeout_seconds: int | None = None,\n    environment_variables: dict[str, str] | None = None,\n) -&gt; Sandbox\n</code></pre> <p>Create and start a sandbox, return immediately once backend accepts.</p> <p>Does NOT wait for RUNNING status. Use .wait() to block until ready. If positional args are provided, the first is the command and the rest are its arguments. If no args are provided, uses defaults (tail -f /dev/null).</p> PARAMETER DESCRIPTION <code>*args</code> <p>Optional command and arguments (e.g., \"echo\", \"hello\", \"world\"). If omitted, uses default command from SandboxDefaults.</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> <code>container_image</code> <p>Container image to use</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>defaults</code> <p>Optional SandboxDefaults to apply</p> <p> TYPE: <code>SandboxDefaults | None</code> DEFAULT: <code>None</code> </p> <code>request_timeout_seconds</code> <p>Timeout for API requests (client-side)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>max_lifetime_seconds</code> <p>Max sandbox lifetime (server-side)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Optional tags for the sandbox</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Optional list of runway IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Optional list of tower IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resource requests (CPU, memory, GPU)</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>mounted_files</code> <p>Files to mount into the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>s3_mount</code> <p>S3 bucket mount configuration</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ports</code> <p>Port mappings for the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>service</code> <p>Service configuration for network access</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>max_timeout_seconds</code> <p>Maximum timeout for sandbox operations</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>environment_variables</code> <p>Environment variables to inject into the sandbox. Merges with and overrides matching keys from the session defaults. Use for non-sensitive config only.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <p>Returns:     A Sandbox instance (start request sent, but may still be starting)</p> Example <pre><code># Using defaults (tail -f /dev/null)\nsb = Sandbox.run()\n\n# Fire and forget style\nsb = Sandbox.run(\"echo\", \"hello\")\n# sb.sandbox_id is set, but sandbox may still be starting\n\n# Wait for ready if needed\nsb = Sandbox.run(\"sleep\", \"infinity\").wait()\nresult = sb.exec([\"echo\", \"hello\"]).result()\n\n# Or use context manager for automatic cleanup\nwith Sandbox.run(\"sleep\", \"infinity\") as sb:\n    result = sb.exec([\"echo\", \"hello\"]).result()\n</code></pre>"},{"location":"api/#aviato.Sandbox.session","title":"session  <code>classmethod</code>","text":"<pre><code>session(defaults: SandboxDefaults | None = None) -&gt; Session\n</code></pre> <p>Create a session for managing multiple sandboxes.</p> <p>Sessions provide: - Shared configuration via defaults - Automatic cleanup of orphaned sandboxes - Function execution via @session.function() decorator</p> PARAMETER DESCRIPTION <code>defaults</code> <p>Optional defaults to apply to sandboxes created via session</p> <p> TYPE: <code>SandboxDefaults | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Session</code> <p>A Session instance</p> Example <pre><code>session = Sandbox.session(defaults)\nsb = session.create(command=\"sleep\", args=[\"infinity\"])\n\n@session.function()\ndef compute(x, y):\n    return x + y\n\nawait session.close()\n</code></pre>"},{"location":"api/#aviato.Sandbox.list","title":"list  <code>classmethod</code>","text":"<pre><code>list(\n    *,\n    tags: list[str] | None = None,\n    status: str | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    base_url: str | None = None,\n    timeout_seconds: float | None = None,\n) -&gt; OperationRef[list[Sandbox]]\n</code></pre> <p>List existing sandboxes with optional filters.</p> <p>Returns OperationRef that resolves to Sandbox instances usable for operations like exec(), stop(), get_status(), read_file(), write_file().</p> PARAMETER DESCRIPTION <code>tags</code> <p>Filter by tags (sandboxes must have ALL specified tags)</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Filter by status (\"running\", \"completed\", \"failed\", etc.)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Filter by runway IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Filter by tower IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>base_url</code> <p>Override API URL (default: AVIATO_BASE_URL env or default)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>timeout_seconds</code> <p>Request timeout (default: 300s)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OperationRef[list[Sandbox]]</code> <p>OperationRef[list[Sandbox]]: Use .result() to block for results,</p> <code>OperationRef[list[Sandbox]]</code> <p>or await directly in async contexts.</p> Example <pre><code># Sync usage\nsandboxes = Sandbox.list(tags=[\"my-batch-job\"]).result()\nfor sb in sandboxes:\n    print(f\"{sb.sandbox_id}: {sb.status}\")\n    sb.stop().result()\n\n# Async usage\nsandboxes = await Sandbox.list(status=\"running\")\nfor sb in sandboxes:\n    result = await sb.exec([\"echo\", \"hello\"])\n</code></pre>"},{"location":"api/#aviato.Sandbox.from_id","title":"from_id  <code>classmethod</code>","text":"<pre><code>from_id(\n    sandbox_id: str,\n    *,\n    base_url: str | None = None,\n    timeout_seconds: float | None = None,\n) -&gt; OperationRef[Sandbox]\n</code></pre> <p>Attach to an existing sandbox by ID.</p> <p>Creates a Sandbox instance connected to an existing sandbox, allowing operations like exec(), stop(), get_status(), etc.</p> PARAMETER DESCRIPTION <code>sandbox_id</code> <p>The ID of the existing sandbox</p> <p> TYPE: <code>str</code> </p> <code>base_url</code> <p>Override API URL (default: AVIATO_BASE_URL env or default)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>timeout_seconds</code> <p>Request timeout (default: 300s)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OperationRef[Sandbox]</code> <p>OperationRef[Sandbox]: Use .result() to block for the Sandbox instance,</p> <code>OperationRef[Sandbox]</code> <p>or await directly in async contexts.</p> RAISES DESCRIPTION <code>SandboxNotFoundError</code> <p>If sandbox doesn't exist</p> Example <pre><code># Sync usage\nsb = Sandbox.from_id(\"sandbox-abc123\").result()\nresult = sb.exec([\"python\", \"-c\", \"print('hello')\"]).result()\nsb.stop().result()\n\n# Async usage\nsb = await Sandbox.from_id(\"sandbox-abc123\")\nresult = await sb.exec([\"python\", \"-c\", \"print('hello')\"])\n</code></pre>"},{"location":"api/#aviato.Sandbox.delete","title":"delete  <code>classmethod</code>","text":"<pre><code>delete(\n    sandbox_id: str,\n    *,\n    base_url: str | None = None,\n    timeout_seconds: float | None = None,\n    missing_ok: bool = False,\n) -&gt; OperationRef[None]\n</code></pre> <p>Delete a sandbox by ID without creating a Sandbox instance.</p> <p>This is a convenience method for cleanup scenarios where you don't need to perform other operations on the sandbox.</p> PARAMETER DESCRIPTION <code>sandbox_id</code> <p>The sandbox ID to delete</p> <p> TYPE: <code>str</code> </p> <code>base_url</code> <p>Override API URL (default: AVIATO_BASE_URL env or default)</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>timeout_seconds</code> <p>Request timeout (default: 300s)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>missing_ok</code> <p>If True, suppress SandboxNotFoundError when sandbox doesn't exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>OperationRef[None]</code> <p>OperationRef[None]: Use .result() to block until complete.</p> <code>OperationRef[None]</code> <p>Raises SandboxNotFoundError if not found (unless missing_ok=True),</p> <code>OperationRef[None]</code> <p>SandboxError if deletion failed.</p> RAISES DESCRIPTION <code>SandboxNotFoundError</code> <p>If sandbox doesn't exist and missing_ok=False</p> <code>SandboxError</code> <p>If deletion failed for other reasons</p> Example <pre><code># Sync usage\nSandbox.delete(\"sandbox-abc123\").result()\n\n# Ignore if already deleted\nSandbox.delete(\"sandbox-abc123\", missing_ok=True).result()\n\n# Async usage\nawait Sandbox.delete(\"sandbox-abc123\")\n</code></pre>"},{"location":"api/#aviato.Sandbox.get_status","title":"get_status","text":"<pre><code>get_status() -&gt; SandboxStatus\n</code></pre> <p>Get the current status of the sandbox from the backend.</p> RETURNS DESCRIPTION <code>SandboxStatus</code> <p>SandboxStatus enum value</p> RAISES DESCRIPTION <code>SandboxNotRunningError</code> <p>If sandbox has not been started</p> Example <pre><code>sb = Sandbox.run(\"sleep\", \"10\")\nstatus = sb.get_status()\nprint(f\"Sandbox is {status}\")  # SandboxStatus.PENDING or RUNNING\n</code></pre>"},{"location":"api/#aviato.Sandbox.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Send StartSandbox to backend, return once accepted.</p> <p>Does NOT wait for RUNNING status. Use wait() to block until ready.</p> Example <pre><code>sandbox = Sandbox(command=\"sleep\", args=[\"infinity\"])\nsandbox.start()\nprint(f\"Started sandbox: {sandbox.sandbox_id}\")\nsandbox.wait()  # Block until RUNNING\n</code></pre>"},{"location":"api/#aviato.Sandbox.wait","title":"wait","text":"<pre><code>wait(timeout: float | None = None) -&gt; Sandbox\n</code></pre> <p>Block until sandbox reaches RUNNING or a terminal state.</p> <p>Returns when sandbox is RUNNING or has already completed (COMPLETED/UNSPECIFIED).</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Maximum seconds to wait. None means use default timeout.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Sandbox</code> <p>Self for method chaining. Check .status to determine final state.</p> RAISES DESCRIPTION <code>SandboxFailedError</code> <p>If sandbox fails to start</p> <code>SandboxTerminatedError</code> <p>If sandbox was terminated externally</p> <code>SandboxTimeoutError</code> <p>If timeout expires</p> Example <pre><code>sb = Sandbox.run(\"sleep\", \"infinity\").wait()\nresult = sb.exec([\"echo\", \"ready\"]).result()\n</code></pre>"},{"location":"api/#aviato.Sandbox.wait_until_complete","title":"wait_until_complete","text":"<pre><code>wait_until_complete(\n    timeout: float | None = None,\n    *,\n    raise_on_termination: bool = True,\n) -&gt; Sandbox\n</code></pre> <p>Block until sandbox reaches terminal state (COMPLETED/FAILED/TERMINATED).</p> <p>After this returns successfully, returncode will be available.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Maximum seconds to wait. None means use default timeout.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>raise_on_termination</code> <p>If True (default), raises SandboxTerminatedError if sandbox was terminated externally.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Sandbox</code> <p>Self for method chaining.</p> RAISES DESCRIPTION <code>SandboxTimeoutError</code> <p>If timeout expires</p> <code>SandboxTerminatedError</code> <p>If sandbox was terminated (and raise_on_termination=True)</p> <code>SandboxFailedError</code> <p>If sandbox failed</p> Example <pre><code>sb = Sandbox.run(\"python\", \"-c\", \"print('done')\")\nsb.wait_until_complete()\nprint(f\"Exit code: {sb.returncode}\")\n</code></pre>"},{"location":"api/#aviato.Sandbox.stop","title":"stop","text":"<pre><code>stop(\n    *,\n    snapshot_on_stop: bool = False,\n    graceful_shutdown_seconds: float = DEFAULT_GRACEFUL_SHUTDOWN_SECONDS,\n    missing_ok: bool = False,\n) -&gt; OperationRef[None]\n</code></pre> <p>Stop sandbox, return OperationRef immediately.</p> <p>The sandbox is deregistered from its session regardless of whether the stop was successful, since the sandbox is no longer usable.</p> PARAMETER DESCRIPTION <code>snapshot_on_stop</code> <p>If True, capture sandbox state before shutdown.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>graceful_shutdown_seconds</code> <p>Time to wait for graceful shutdown.</p> <p> TYPE: <code>float</code> DEFAULT: <code>DEFAULT_GRACEFUL_SHUTDOWN_SECONDS</code> </p> <code>missing_ok</code> <p>If True, suppress SandboxNotFoundError when sandbox doesn't exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>OperationRef[None]</code> <p>OperationRef[None]: Use .result() to block until complete.</p> <code>OperationRef[None]</code> <p>Raises SandboxError on failure, SandboxNotFoundError if not found</p> <code>OperationRef[None]</code> <p>(unless missing_ok=True).</p> Example <pre><code>sb.stop().result()  # Block until stopped\n\n# Ignore if already deleted\nsb.stop(missing_ok=True).result()\n</code></pre>"},{"location":"api/#aviato.Sandbox.exec","title":"exec","text":"<pre><code>exec(\n    command: Sequence[str],\n    *,\n    cwd: str | None = None,\n    check: bool = False,\n    timeout_seconds: float | None = None,\n) -&gt; Process\n</code></pre> <p>Execute command, return Process immediately.</p> <p>Note: If sandbox is not yet RUNNING, this method waits for it first. The timeout_seconds parameter only applies to command execution, not to the initial wait for RUNNING status.</p> PARAMETER DESCRIPTION <code>command</code> <p>Command and arguments to execute</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>cwd</code> <p>Working directory for command execution. Must be an absolute path. When specified, the command is wrapped with a shell cd.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>If True, raise SandboxExecutionError on non-zero returncode</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>timeout_seconds</code> <p>Timeout for command execution (after sandbox is RUNNING). Does not include time waiting for sandbox to reach RUNNING status.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Process</code> <p>Process handle with streaming stdout/stderr. Call .result() to block</p> <code>Process</code> <p>for the final ProcessResult, or iterate over .stdout/.stderr for</p> <code>Process</code> <p>real-time output.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If command is empty or cwd is invalid (empty or relative path)</p> Example <pre><code># Get result directly\nprocess = sb.exec([\"echo\", \"hello\"])\nresult = process.result()\nprint(result.stdout)\n\n# With working directory\nresult = sb.exec([\"ls\", \"-la\"], cwd=\"/app\").result()\n\n# Stream output in real-time\nprocess = sb.exec([\"python\", \"script.py\"])\nfor line in process.stdout:\n    print(line)\nresult = process.result()\n\n# Async usage\nresult = await sb.exec([\"echo\", \"hello\"])\n</code></pre>"},{"location":"api/#aviato.Sandbox.read_file","title":"read_file","text":"<pre><code>read_file(\n    filepath: str, *, timeout_seconds: float | None = None\n) -&gt; OperationRef[bytes]\n</code></pre> <p>Read file from sandbox, return OperationRef immediately.</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to file in sandbox</p> <p> TYPE: <code>str</code> </p> <code>timeout_seconds</code> <p>Timeout for the operation</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OperationRef[bytes]</code> <p>OperationRef[bytes]: Use .result() to block and retrieve contents.</p> Example <pre><code>data = sb.read_file(\"/output/result.txt\").result()\n</code></pre>"},{"location":"api/#aviato.Sandbox.write_file","title":"write_file","text":"<pre><code>write_file(\n    filepath: str,\n    contents: bytes,\n    *,\n    timeout_seconds: float | None = None,\n) -&gt; OperationRef[None]\n</code></pre> <p>Write file to sandbox, return OperationRef immediately.</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to file in sandbox</p> <p> TYPE: <code>str</code> </p> <code>contents</code> <p>File contents as bytes</p> <p> TYPE: <code>bytes</code> </p> <code>timeout_seconds</code> <p>Timeout for the operation</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OperationRef[None]</code> <p>OperationRef[None]: Use .result() to block until complete.</p> Example <pre><code>sb.write_file(\"/input/data.txt\", b\"content\").result()\n</code></pre>"},{"location":"api/#aviato.SandboxStatus","title":"SandboxStatus","text":"<p>Sandbox status values.</p>"},{"location":"api/#aviato.SandboxStatus.from_proto","title":"from_proto  <code>classmethod</code>","text":"<pre><code>from_proto(proto_status: int) -&gt; SandboxStatus\n</code></pre> <p>Convert protobuf status enum to SandboxStatus.</p>"},{"location":"api/#aviato.SandboxStatus.to_proto","title":"to_proto","text":"<pre><code>to_proto() -&gt; int\n</code></pre> <p>Convert SandboxStatus to protobuf enum</p>"},{"location":"api/#aviato.Session","title":"Session","text":"<pre><code>Session(\n    defaults: SandboxDefaults | None = None,\n    report_to: list[str] | None = None,\n)\n</code></pre> <p>Manages sandbox lifecycle and provides function execution.</p> <p>Use a session when: - Creating multiple sandboxes with shared configuration - Executing Python functions in sandboxes - You want automatic cleanup of orphaned sandboxes</p> PARAMETER DESCRIPTION <code>defaults</code> <p>Sandbox configuration defaults to apply to all sandboxes created by this session.</p> <p> TYPE: <code>SandboxDefaults | None</code> DEFAULT: <code>None</code> </p> <code>report_to</code> <p>Controls metrics reporting integrations. - None (default): Auto-detect. Logs to wandb if WANDB_API_KEY is set   and an active wandb run exists. - []: Disable all reporting. - [\"wandb\"]: Explicitly enable wandb reporting.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <p>Metrics are automatically tracked when exec() completes on any sandbox associated with this session. Use log_metrics(step=N) to log metrics at specific training steps.</p> Example <pre><code>defaults = SandboxDefaults(container_image=\"python:3.11\")\n\n# Sync context manager\nwith Session(defaults) as session:\n    # Create and start sandboxes with session defaults\n    sb1 = session.sandbox(command=\"sleep\", args=[\"infinity\"])\n    sb2 = session.sandbox(command=\"sleep\", args=[\"infinity\"])\n\n    # Execute commands - metrics tracked automatically\n    result = sb1.exec([\"echo\", \"hello\"]).result()\n\n    # Execute functions in sandboxes\n    @session.function()\n    def compute(x: int, y: int) -&gt; int:\n        return x + y\n\n    result = compute.remote(2, 3).result()  # Returns OperationRef\n    print(result)  # 5\n\n# Session automatically cleans up all sandboxes on exit\n\n# Async context manager also supported\nasync with Session(defaults) as session:\n    sb = session.sandbox(command=\"sleep\", args=[\"infinity\"])\n    result = await sb.exec([\"echo\", \"hello\"])\n\n# Explicit wandb reporting with step correlation\nwith Session(defaults, report_to=[\"wandb\"]) as session:\n    for step in range(100):\n        sb = session.sandbox(...)\n        result = sb.exec(...).result()  # Metrics tracked automatically\n        session.log_metrics(step=step)  # Log at training step\n\n# Disable all reporting\nwith Session(defaults, report_to=[]) as session:\n    sb = session.sandbox(...)\n</code></pre>"},{"location":"api/#aviato.Session.sandbox_count","title":"sandbox_count  <code>property</code>","text":"<pre><code>sandbox_count: int\n</code></pre> <p>Number of sandboxes currently tracked by this session.</p>"},{"location":"api/#aviato.Session.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(\n    step: int | None = None, reset: bool = True\n) -&gt; bool\n</code></pre> <p>Log accumulated sandbox metrics to wandb.</p> <p>Call this during training to correlate sandbox usage with training steps. Metrics are automatically tracked when exec() completes, so users only need to call log_metrics() for step correlation.</p> <p>Metrics are also logged automatically on session close.</p> PARAMETER DESCRIPTION <code>step</code> <p>Training step to associate with metrics. If provided, metrics are logged at this step number in wandb.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>reset</code> <p>If True (default), reset accumulated metrics after logging. Set to False to keep accumulating.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if metrics were logged, False if no reporter configured</p> <code>bool</code> <p>or no active wandb run.</p> Example <pre><code>with Session(defaults, report_to=[\"wandb\"]) as session:\n    for step in range(100):\n        sb = session.sandbox(...)\n        result = sb.exec(...).result()  # Metrics tracked automatically\n        session.log_metrics(step=step)  # Log at each step\n</code></pre>"},{"location":"api/#aviato.Session.close","title":"close","text":"<pre><code>close() -&gt; OperationRef[None]\n</code></pre> <p>Stop all managed sandboxes, return OperationRef immediately.</p> RETURNS DESCRIPTION <code>OperationRef[None]</code> <p>OperationRef[None]: Use .result() to block until all sandboxes stopped.</p> RAISES DESCRIPTION <code>SandboxError</code> <p>If one or more running sandboxes failed to stop.</p> Example <pre><code>session.close().result()  # Block until all sandboxes stopped\n</code></pre>"},{"location":"api/#aviato.Session.sandbox","title":"sandbox","text":"<pre><code>sandbox(\n    *,\n    command: str | None = None,\n    args: list[str] | None = None,\n    container_image: str | None = None,\n    tags: list[str] | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    resources: dict[str, Any] | None = None,\n    mounted_files: list[dict[str, Any]] | None = None,\n    s3_mount: dict[str, Any] | None = None,\n    ports: list[dict[str, Any]] | None = None,\n    service: dict[str, Any] | None = None,\n    max_timeout_seconds: int | None = None,\n    environment_variables: dict[str, str] | None = None,\n) -&gt; Sandbox\n</code></pre> <p>Create and start a sandbox with session defaults, return immediately.</p> <p>This is the recommended way to create sandboxes in the sync API. The sandbox is created and started, returning immediately once the backend accepts the start request (does NOT wait for RUNNING status).</p> PARAMETER DESCRIPTION <code>command</code> <p>Command to run in sandbox</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>args</code> <p>Arguments for the command</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>container_image</code> <p>Container image to use</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags for the sandbox (merged with session defaults)</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Optional list of runway IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Optional list of tower IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resource requests (CPU, memory, GPU)</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>mounted_files</code> <p>Files to mount into the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>s3_mount</code> <p>S3 bucket mount configuration</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ports</code> <p>Port mappings for the sandbox</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>service</code> <p>Service configuration for network access</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>max_timeout_seconds</code> <p>Maximum timeout for sandbox operations</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>environment_variables</code> <p>Environment variables to inject into the sandbox. Merges with and overrides matching keys from the session defaults. Use for non-sensitive config only.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Sandbox</code> <p>A started Sandbox instance. Use .wait() to block until RUNNING.</p> RAISES DESCRIPTION <code>SandboxError</code> <p>If the session has been closed.</p> Example <pre><code>with Session(defaults) as session:\n    sb = session.sandbox(command=\"sleep\", args=[\"infinity\"])\n    sb.wait()  # Optional: block until RUNNING\n    result = sb.exec([\"echo\", \"hello\"]).result()\n</code></pre>"},{"location":"api/#aviato.Session.list","title":"list","text":"<pre><code>list(\n    *,\n    tags: list[str] | None = None,\n    status: str | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    adopt: bool = False,\n) -&gt; OperationRef[list[Sandbox]]\n</code></pre> <p>List sandboxes, optionally adopting them into this session.</p> <p>Automatically includes the session's default tags in the filter. This makes it easy to find sandboxes created by this session or a previous run with the same defaults.</p> PARAMETER DESCRIPTION <code>tags</code> <p>Additional tags to filter by (merged with session's default tags)</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Filter by status</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Filter by runway IDs (defaults to session's runway_ids if set)</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Filter by tower IDs (defaults to session's tower_ids if set)</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>adopt</code> <p>If True, register discovered sandboxes with this session    so they are stopped when the session closes</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>OperationRef[list[Sandbox]]</code> <p>OperationRef[list[Sandbox]]: Use .result() to block for results,</p> <code>OperationRef[list[Sandbox]]</code> <p>or await directly in async contexts.</p> Example <pre><code># Session defaults include a tag for this application/run\ndefaults = SandboxDefaults(tags=(\"my-app\", \"run-abc123\"))\n\nwith Session(defaults) as session:\n    # Sync usage - automatically filters by [\"my-app\", \"run-abc123\"]\n    orphans = session.list(adopt=True).result()\n\n    # Can add additional filters\n    running = session.list(status=\"running\").result()\n\n# Async usage\nasync with Session(defaults) as session:\n    orphans = await session.list(adopt=True)\n</code></pre>"},{"location":"api/#aviato.Session.from_id","title":"from_id","text":"<pre><code>from_id(\n    sandbox_id: str, *, adopt: bool = True\n) -&gt; OperationRef[Sandbox]\n</code></pre> <p>Attach to an existing sandbox, optionally adopting it into this session.</p> PARAMETER DESCRIPTION <code>sandbox_id</code> <p>The ID of the existing sandbox</p> <p> TYPE: <code>str</code> </p> <code>adopt</code> <p>If True (default), register the sandbox with this session</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OperationRef[Sandbox]</code> <p>OperationRef[Sandbox]: Use .result() to block for the Sandbox instance,</p> <code>OperationRef[Sandbox]</code> <p>or await directly in async contexts.</p> Example <pre><code>with Session(defaults) as session:\n    # Sync usage - reconnect to a sandbox\n    sb = session.from_id(\"sandbox-abc123\").result()\n    result = sb.exec([\"echo\", \"hello\"]).result()\n# sb is stopped when session exits\n\n# Async usage\nasync with Session(defaults) as session:\n    sb = await session.from_id(\"sandbox-abc123\")\n    result = await sb.exec([\"echo\", \"hello\"])\n</code></pre>"},{"location":"api/#aviato.Session.adopt","title":"adopt","text":"<pre><code>adopt(sandbox: Sandbox) -&gt; None\n</code></pre> <p>Adopt an existing Sandbox instance into this session for cleanup tracking.</p> <p>Use this when you have a Sandbox from Sandbox.list() or Sandbox.from_id() that you want to be automatically stopped when the session closes.</p> PARAMETER DESCRIPTION <code>sandbox</code> <p>A Sandbox instance to track</p> <p> TYPE: <code>Sandbox</code> </p> RAISES DESCRIPTION <code>SandboxError</code> <p>If the session is closed</p> <code>ValueError</code> <p>If the sandbox has no sandbox_id</p> Example <pre><code>with Session(defaults) as session:\n    # Get sandboxes via class method\n    sandboxes = Sandbox.list(tags=[\"my-job\"]).result()\n\n    # Adopt them into the session\n    for sb in sandboxes:\n        session.adopt(sb)\n\n    # Now they'll be stopped when session closes\n</code></pre>"},{"location":"api/#aviato.Session.function","title":"function","text":"<pre><code>function(\n    *,\n    container_image: str | None = None,\n    serialization: Serialization = JSON,\n    temp_dir: str | None = None,\n    runway_ids: list[str] | None = None,\n    tower_ids: list[str] | None = None,\n    resources: dict[str, Any] | None = None,\n    mounted_files: Sequence[dict[str, Any]] | None = None,\n    s3_mount: dict[str, Any] | None = None,\n    ports: Sequence[dict[str, Any]] | None = None,\n    service: dict[str, Any] | None = None,\n    max_timeout_seconds: int | None = None,\n    environment_variables: dict[str, str] | None = None,\n) -&gt; Callable[[Callable[P, R]], RemoteFunction[P, R]]\n</code></pre> <p>Decorator to execute a Python function in a sandbox.</p> <p>Each function call creates an ephemeral sandbox, executes the function, and returns the result. The sandbox is automatically cleaned up.</p> <p>The decorated function must be synchronous. Async functions are not supported.</p> PARAMETER DESCRIPTION <code>container_image</code> <p>Override session's default image for this function</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>serialization</code> <p>How to serialize arguments and return values. Defaults to JSON for safety. Use PICKLE for complex types, but only in trusted environments.</p> <p> TYPE: <code>Serialization</code> DEFAULT: <code>JSON</code> </p> <code>temp_dir</code> <p>Override temp directory for payload/result files in sandbox. Defaults to session default. Created if missing.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>runway_ids</code> <p>Optional list of runway IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>tower_ids</code> <p>Optional list of tower IDs</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resource requests (CPU, memory, GPU)</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>mounted_files</code> <p>Files to mount into the sandbox</p> <p> TYPE: <code>Sequence[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>s3_mount</code> <p>S3 bucket mount configuration</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ports</code> <p>Port mappings for the sandbox</p> <p> TYPE: <code>Sequence[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>service</code> <p>Service configuration for network access</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>max_timeout_seconds</code> <p>Maximum timeout for sandbox operations</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>environment_variables</code> <p>Environment variables to inject into the sandbox. Merges with and overrides matching keys from the session defaults. Use for non-sensitive config only.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable[P, R]], RemoteFunction[P, R]]</code> <p>A decorator that wraps a function as a RemoteFunction</p> Example <pre><code>with Session(defaults) as session:\n    @session.function()\n    def compute(x: int, y: int) -&gt; int:\n        return x + y\n\n    @session.function(serialization=Serialization.PICKLE)\n    def process_complex(data: MyClass) -&gt; MyClass:\n        return data.transform()\n\n    # Call .remote() to execute in sandbox\n    ref = compute.remote(2, 3)  # Returns OperationRef immediately\n    result = ref.result()       # Block for result\n    print(result)  # 5\n\n    # Or use await in async context\n    result = await compute.remote(2, 3)\n\n    # Execute locally for testing\n    result = compute.local(2, 3)\n\n    # Map over multiple inputs in parallel\n    refs = compute.map([(1, 2), (3, 4), (5, 6)])\n    results = [ref.result() for ref in refs]\n</code></pre>"},{"location":"api/#aviato.OperationRef","title":"OperationRef","text":"<pre><code>OperationRef(future: Future[T])\n</code></pre> <p>Generic ref for async operations with lazy result retrieval.</p> <p>OperationRef wraps a concurrent.futures.Future and provides a unified interface for both synchronous and asynchronous result retrieval. This enables the sync/async hybrid API where operations return immediately and results are retrieved lazily.</p>                CLASS TYPE PARAMETER              DESCRIPTION <code>T</code> <p>The type of the result this operation will return.</p> <p> </p> <p>Examples:</p> <p>Synchronous usage: <pre><code>ref = sandbox.read_file(\"/path/to/file\")  # Returns OperationRef[bytes]\ndata = ref.result()  # Block until complete\n</code></pre></p> <p>With timeout: <pre><code>try:\n    data = ref.result(timeout=5.0)\nexcept concurrent.futures.TimeoutError:\n    print(\"Operation timed out\")\n</code></pre></p> <p>Async usage: <pre><code>data = await ref  # Awaitable in async context\n</code></pre></p> <p>Initialize with a concurrent.futures.Future.</p> PARAMETER DESCRIPTION <code>future</code> <p>The underlying future that will contain the result.</p> <p> TYPE: <code>Future[T]</code> </p>"},{"location":"api/#aviato.OperationRef.result","title":"result","text":"<pre><code>result(timeout: float | None = None) -&gt; T\n</code></pre> <p>Block until the result is ready and return it.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Maximum seconds to wait. None means wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The result of the operation.</p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If timeout expires before completion.</p> <code>CancelledError</code> <p>If the operation was cancelled.</p> <code>Exception</code> <p>Any exception raised by the operation.</p>"},{"location":"api/#aviato.Process","title":"Process","text":"<pre><code>Process(\n    future: Future[ProcessResult],\n    command: list[str],\n    stdout: StreamReader,\n    stderr: StreamReader,\n    stats_callback: Callable[\n        [ProcessResult | None, BaseException | None], None\n    ]\n    | None = None,\n)\n</code></pre> <p>Handle for a running process with streaming stdout/stderr.</p> <p>Process inherits from OperationRef[ProcessResult] and adds streaming capabilities and process-specific methods. It wraps an async operation that executes a command in a sandbox.</p> <p>The process's output streams (stdout, stderr) can be iterated either synchronously or asynchronously. The result() method blocks until completion and returns the full ProcessResult.</p> ATTRIBUTE DESCRIPTION <code>stdout</code> <p>StreamReader for standard output</p> <p> </p> <code>stderr</code> <p>StreamReader for standard error</p> <p> </p> <p>Examples:</p> <p>Basic execution with result: <pre><code>process = sandbox.exec([\"echo\", \"hello\"])\nresult = process.result()\nprint(result.stdout)  # hello\n</code></pre></p> <p>Streaming output: <pre><code>process = sandbox.exec([\"python\", \"-c\", \"print('line1'); print('line2')\"])\nfor line in process.stdout:\n    print(f\"Got: {line}\")\n</code></pre></p> <p>Async streaming: <pre><code>async for line in process.stdout:\n    print(f\"Got: {line}\")\n</code></pre></p> <p>Waiting with timeout: <pre><code>try:\n    exit_code = process.wait(timeout=10.0)\nexcept concurrent.futures.TimeoutError:\n    process.cancel()\n</code></pre></p> <p>Initialize with a future and stream readers.</p> PARAMETER DESCRIPTION <code>future</code> <p>Future that will contain the ProcessResult when complete.</p> <p> TYPE: <code>Future[ProcessResult]</code> </p> <code>command</code> <p>The command being executed.</p> <p> TYPE: <code>list[str]</code> </p> <code>stdout</code> <p>StreamReader for stdout.</p> <p> TYPE: <code>StreamReader</code> </p> <code>stderr</code> <p>StreamReader for stderr.</p> <p> TYPE: <code>StreamReader</code> </p> <code>stats_callback</code> <p>Optional callback invoked once on completion with (result, None) on success or (None, exception) on error.</p> <p> TYPE: <code>Callable[[ProcessResult | None, BaseException | None], None] | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/#aviato.Process.returncode","title":"returncode  <code>property</code>","text":"<pre><code>returncode: int | None\n</code></pre> <p>The process exit code, or None if not yet complete.</p>"},{"location":"api/#aviato.Process.command","title":"command  <code>property</code>","text":"<pre><code>command: list[str]\n</code></pre> <p>The command that was executed.</p>"},{"location":"api/#aviato.Process.poll","title":"poll","text":"<pre><code>poll() -&gt; int | None\n</code></pre> <p>Check if the process has completed without blocking.</p> RETURNS DESCRIPTION <code>int | None</code> <p>The exit code if the process has completed, None otherwise.</p>"},{"location":"api/#aviato.Process.wait","title":"wait","text":"<pre><code>wait(timeout: float | None = None) -&gt; int\n</code></pre> <p>Block until the process completes.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Maximum seconds to wait. None means wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The process exit code.</p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If timeout expires.</p> <code>CancelledError</code> <p>If the operation was cancelled.</p> <code>Exception</code> <p>Any exception from the execution.</p>"},{"location":"api/#aviato.Process.result","title":"result","text":"<pre><code>result(timeout: float | None = None) -&gt; ProcessResult\n</code></pre> <p>Block until complete and return the full ProcessResult.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Maximum seconds to wait. None means wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ProcessResult</code> <p>The ProcessResult containing stdout, stderr, and exit code.</p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If timeout expires.</p> <code>CancelledError</code> <p>If the operation was cancelled.</p> <code>Exception</code> <p>Any exception from the execution.</p>"},{"location":"api/#aviato.Process.cancel","title":"cancel","text":"<pre><code>cancel() -&gt; bool\n</code></pre> <p>Attempt to cancel the process.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if successfully cancelled, False otherwise.</p>"},{"location":"api/#aviato.ProcessResult","title":"ProcessResult  <code>dataclass</code>","text":"<pre><code>ProcessResult(\n    stdout: str,\n    stderr: str,\n    returncode: int,\n    stdout_bytes: bytes = b\"\",\n    stderr_bytes: bytes = b\"\",\n    command: list[str] = list(),\n)\n</code></pre> <p>Result from a completed streaming exec operation.</p> <p>Contains both the raw bytes and decoded strings for stdout/stderr, along with the exit code and original command.</p> ATTRIBUTE DESCRIPTION <code>stdout</code> <p>Decoded stdout as UTF-8 string</p> <p> TYPE: <code>str</code> </p> <code>stderr</code> <p>Decoded stderr as UTF-8 string</p> <p> TYPE: <code>str</code> </p> <code>returncode</code> <p>Exit code from the command (0 = success)</p> <p> TYPE: <code>int</code> </p> <code>stdout_bytes</code> <p>Raw stdout bytes</p> <p> TYPE: <code>bytes</code> </p> <code>stderr_bytes</code> <p>Raw stderr bytes</p> <p> TYPE: <code>bytes</code> </p> <code>command</code> <p>The command that was executed</p> <p> TYPE: <code>list[str]</code> </p> <p>Examples:</p> <pre><code>result = process.result()\nif result.returncode == 0:\n    print(result.stdout)\nelse:\n    print(f\"Error: {result.stderr}\")\n</code></pre>"},{"location":"api/#aviato.Serialization","title":"Serialization","text":"<p>Serialization modes for sandbox function execution.</p>"},{"location":"api/#aviato.StreamReader","title":"StreamReader","text":"<pre><code>StreamReader(\n    queue: Queue[str | None], loop_manager: _LoopManager\n)\n</code></pre> <p>Sync and async iterable for streaming output.</p> <p>StreamReader wraps an asyncio.Queue and provides both synchronous and asynchronous iteration interfaces. This enables streaming output to be consumed in both sync and async contexts.</p> <p>The stream uses None as a sentinel value to signal end-of-stream.</p> <p>Examples:</p> <p>Synchronous iteration: <pre><code>for line in process.stdout:\n    print(line)\n</code></pre></p> <p>Asynchronous iteration: <pre><code>async for line in process.stdout:\n    print(line)\n</code></pre></p> <p>Initialize with a queue and loop manager.</p> PARAMETER DESCRIPTION <code>queue</code> <p>The asyncio.Queue to read from.</p> <p> TYPE: <code>Queue[str | None]</code> </p> <code>loop_manager</code> <p>The _LoopManager for executing async operations.</p> <p> TYPE: <code>_LoopManager</code> </p>"},{"location":"api/#aviato.AsyncFunctionError","title":"AsyncFunctionError","text":"<p>Raised when an async function is passed to @session.function().</p> <p>Async functions are not supported because the sandbox executes Python synchronously. The decorated function must be a regular (sync) function.</p>"},{"location":"api/#aviato.AviatoAuthenticationError","title":"AviatoAuthenticationError","text":"<p>Raised when authentication fails.</p>"},{"location":"api/#aviato.AviatoError","title":"AviatoError","text":"<p>Base exception for all Aviato operations.</p>"},{"location":"api/#aviato.FunctionError","title":"FunctionError","text":"<p>Base exception for function execution operations.</p>"},{"location":"api/#aviato.FunctionSerializationError","title":"FunctionSerializationError","text":"<p>Raised when arguments, referenced globals, or closures cannot be serialized.</p>"},{"location":"api/#aviato.SandboxError","title":"SandboxError","text":"<p>Base exception for sandbox operations.</p>"},{"location":"api/#aviato.SandboxExecutionError","title":"SandboxExecutionError","text":"<pre><code>SandboxExecutionError(\n    message: str,\n    *,\n    exec_result: ProcessResult | None = None,\n    exception_type: str | None = None,\n    exception_message: str | None = None,\n)\n</code></pre> <p>Raised when command execution fails inside a sandbox.</p> <p>Access execution details via exec_result</p>"},{"location":"api/#aviato.SandboxFailedError","title":"SandboxFailedError","text":"<p>Raised when a sandbox fails to start or encounters a fatal error.</p>"},{"location":"api/#aviato.SandboxFileError","title":"SandboxFileError","text":"<pre><code>SandboxFileError(\n    message: str, *, filepath: str | None = None\n)\n</code></pre> <p>Raised when a file operation fails in the sandbox.</p> <p>This is a sandbox infrastructure error, not a user code error. Inherits from SandboxError since it's a sandbox operation failure.</p>"},{"location":"api/#aviato.SandboxNotFoundError","title":"SandboxNotFoundError","text":"<pre><code>SandboxNotFoundError(\n    message: str, *, sandbox_id: str | None = None\n)\n</code></pre> <p>Raised when a sandbox is not found (e.g., already deleted).</p>"},{"location":"api/#aviato.SandboxNotRunningError","title":"SandboxNotRunningError","text":"<p>Raised when an operation requires a running sandbox.</p>"},{"location":"api/#aviato.SandboxTerminatedError","title":"SandboxTerminatedError","text":"<p>Raised when a sandbox was terminated externally.</p>"},{"location":"api/#aviato.SandboxTimeoutError","title":"SandboxTimeoutError","text":"<p>Raised when a sandbox operation times out.</p>"},{"location":"api/#aviato.WandbAuthError","title":"WandbAuthError","text":"<p>Raised when W&amp;B authentication is misconfigured.</p>"},{"location":"api/#aviato.results","title":"results","text":"<pre><code>results(ref: OperationRef[T]) -&gt; T\n</code></pre><pre><code>results(refs: Sequence[OperationRef[T]]) -&gt; list[T]\n</code></pre> <pre><code>results(\n    refs: OperationRef[T] | Sequence[OperationRef[T]],\n) -&gt; T | list[T]\n</code></pre> <p>Block for one or more OperationRefs and return results.</p> <p>This is a convenience function for retrieving results from OperationRefs. For a single ref, returns the result directly. For a sequence of refs, returns a list of results in the same order.</p> PARAMETER DESCRIPTION <code>refs</code> <p>A single OperationRef or a sequence of OperationRefs.</p> <p> TYPE: <code>OperationRef[T] | Sequence[OperationRef[T]]</code> </p> RETURNS DESCRIPTION <code>T | list[T]</code> <p>The result(s) from the operation(s).</p> RAISES DESCRIPTION <code>Exception</code> <p>Any exception raised by the underlying operation(s).</p> <p>Examples:</p> <p>Single ref: <pre><code>data = aviato.results(sandbox.read_file(\"/path\"))\n</code></pre></p> <p>Multiple refs: <pre><code>all_results = aviato.results([sb.read_file(f) for f in files])\n</code></pre></p>"},{"location":"api/#aviato.wait","title":"wait","text":"<pre><code>wait(\n    waitables: Sequence[Waitable],\n    num_returns: int | None = None,\n    timeout: float | None = None,\n) -&gt; tuple[list[Waitable], list[Waitable]]\n</code></pre> <p>Wait for waitables to complete, return (done, pending).</p> <p>Each waitable type has natural \"wait for\" behavior: - Sandbox: waits until RUNNING status - OperationRef: waits until operation completes - Process: waits until process completes</p> PARAMETER DESCRIPTION <code>waitables</code> <p>Sequence of Sandbox, OperationRef, or Process objects.</p> <p> TYPE: <code>Sequence[Waitable]</code> </p> <code>num_returns</code> <p>If specified, return after this many complete. If None, wait for all to complete.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>timeout</code> <p>Maximum seconds to wait. If None, wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[list[Waitable], list[Waitable]]</code> <p>Tuple of (done, pending) lists containing the original waitable objects.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If num_returns is less than 1.</p> <p>Examples:</p> <p>Wait for all sandboxes to be running: <pre><code>sandboxes = [Sandbox.run(...) for _ in range(5)]\ndone, pending = aviato.wait(sandboxes)\n</code></pre></p> <p>Wait for first 2 operations to complete: <pre><code>refs = [sb.read_file(f) for f in files]\ndone, pending = aviato.wait(refs, num_returns=2)\n</code></pre></p> <p>Wait with timeout: <pre><code>done, pending = aviato.wait(procs, timeout=30.0)\n</code></pre></p>"},{"location":"guides/AGENTS/","title":"How-To Guides","text":"<p>Task-oriented guides for common operations. Each guide answers \"How do I...?\"</p>"},{"location":"guides/AGENTS/#files","title":"Files","text":"Guide Task <code>execution.md</code> Run commands with <code>exec()</code> - buffered and streaming modes <code>file-operations.md</code> Read and write files in sandboxes <code>sessions.md</code> Manage multiple sandboxes with shared configuration <code>remote-functions.md</code> Execute Python functions remotely with <code>@session.function()</code> <code>cleanup-patterns.md</code> Resource management and graceful shutdown <code>sync-vs-async.md</code> When to use sync vs async patterns, calling async from sync <code>sandbox-config.md</code> Configure resources, mounted files, ports, timeouts <code>troubleshooting.md</code> Debug common issues - sandbox failures, auth errors, timeouts <code>environment-variables.md</code> Use environment variables in sandboxes <code>swebench.md</code> Run SWE-bench evaluations with parallel Aviato sandboxes <code>rl-training.md</code> RL training with code execution rewards, TRL integration"},{"location":"guides/AGENTS/#key-patterns","title":"Key Patterns","text":"<p>All guides follow these conventions:</p> <ul> <li>Process for exec: <code>exec()</code> returns a <code>Process</code> (inherits from OperationRef) - call <code>.result()</code> to block for the result</li> <li>OperationRef for other ops: <code>read_file()</code>, <code>write_file()</code>, <code>stop()</code> return <code>OperationRef</code> - call <code>.result()</code> to block</li> <li>Context managers: Use <code>with</code> statements for automatic cleanup</li> </ul>"},{"location":"guides/AGENTS/#writing-new-guides","title":"Writing New Guides","text":"<ol> <li>Focus on a single task (\"How do I X?\")</li> <li>Start with the simplest working example</li> <li>Add variations for common needs</li> <li>Include error handling where relevant</li> <li>Link to reference docs for parameter details</li> <li>Use <code>.result()</code> consistently for both Process and OperationRef</li> <li>Update <code>mkdocs.yml</code> nav section to include the new guide</li> </ol>"},{"location":"guides/AGENTS/#relationship-to-other-docs","title":"Relationship to Other Docs","text":"<ul> <li>Examples (<code>examples/</code>): Runnable Python scripts</li> </ul>"},{"location":"guides/CLAUDE/","title":"How-To Guides","text":"<p>Task-oriented guides for common operations. Each guide answers \"How do I...?\"</p>"},{"location":"guides/CLAUDE/#files","title":"Files","text":"Guide Task <code>execution.md</code> Run commands with <code>exec()</code> - buffered and streaming modes <code>file-operations.md</code> Read and write files in sandboxes <code>sessions.md</code> Manage multiple sandboxes with shared configuration <code>remote-functions.md</code> Execute Python functions remotely with <code>@session.function()</code> <code>cleanup-patterns.md</code> Resource management and graceful shutdown <code>sync-vs-async.md</code> When to use sync vs async patterns, calling async from sync <code>sandbox-config.md</code> Configure resources, mounted files, ports, timeouts <code>troubleshooting.md</code> Debug common issues - sandbox failures, auth errors, timeouts <code>environment-variables.md</code> Use environment variables in sandboxes <code>swebench.md</code> Run SWE-bench evaluations with parallel Aviato sandboxes <code>rl-training.md</code> RL training with code execution rewards, TRL integration"},{"location":"guides/CLAUDE/#key-patterns","title":"Key Patterns","text":"<p>All guides follow these conventions:</p> <ul> <li>Process for exec: <code>exec()</code> returns a <code>Process</code> (inherits from OperationRef) - call <code>.result()</code> to block for the result</li> <li>OperationRef for other ops: <code>read_file()</code>, <code>write_file()</code>, <code>stop()</code> return <code>OperationRef</code> - call <code>.result()</code> to block</li> <li>Context managers: Use <code>with</code> statements for automatic cleanup</li> </ul>"},{"location":"guides/CLAUDE/#writing-new-guides","title":"Writing New Guides","text":"<ol> <li>Focus on a single task (\"How do I X?\")</li> <li>Start with the simplest working example</li> <li>Add variations for common needs</li> <li>Include error handling where relevant</li> <li>Link to reference docs for parameter details</li> <li>Use <code>.result()</code> consistently for both Process and OperationRef</li> <li>Update <code>mkdocs.yml</code> nav section to include the new guide</li> </ol>"},{"location":"guides/CLAUDE/#relationship-to-other-docs","title":"Relationship to Other Docs","text":"<ul> <li>Examples (<code>examples/</code>): Runnable Python scripts</li> </ul>"},{"location":"guides/cleanup-patterns/","title":"Cleanup Patterns Guide","text":"<p>This guide covers resource management and cleanup strategies for sandboxes.</p>"},{"location":"guides/cleanup-patterns/#automatic-cleanup","title":"Automatic Cleanup","text":""},{"location":"guides/cleanup-patterns/#context-managers-recommended","title":"Context Managers (Recommended)","text":"<p>Sandboxes are stopped when exiting the context:</p> <pre><code>from aviato import Sandbox\n\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"echo\", \"hello\"]).result()\n# sandbox.stop() called automatically\n</code></pre> <p>Sessions clean up all their sandboxes:</p> <pre><code>import aviato\nfrom aviato import SandboxDefaults\n\nwith aviato.Session(SandboxDefaults(container_image=\"python:3.11\")) as session:\n    sb1 = session.sandbox()\n    sb2 = session.sandbox()\n# Both sandboxes stopped automatically\n</code></pre>"},{"location":"guides/cleanup-patterns/#global-cleanup-handlers","title":"Global Cleanup Handlers","text":"<p>The SDK registers cleanup handlers for process termination:</p> Scenario Behavior Normal script exit <code>atexit</code> handler stops all sandboxes Ctrl+C (SIGINT) Signal handler stops all sandboxes SIGTERM Signal handler stops all sandboxes Second Ctrl+C Force exit (prevents hang)"},{"location":"guides/cleanup-patterns/#manual-cleanup","title":"Manual Cleanup","text":""},{"location":"guides/cleanup-patterns/#stop","title":"stop()","text":"<pre><code>sandbox = Sandbox.run()\nresult = sandbox.exec([\"echo\", \"hello\"]).result()\nsandbox.stop().result()\n\n# With options\nsandbox.stop(graceful_shutdown_seconds=30.0).result()\nsandbox.stop(snapshot_on_stop=True).result()\n</code></pre>"},{"location":"guides/cleanup-patterns/#session-close","title":"Session close()","text":"<pre><code>from aviato import SandboxDefaults\n\nsession = aviato.Session(SandboxDefaults(container_image=\"python:3.11\"))\nsandbox = session.sandbox()\n# ...\nsession.close().result()  # Stops all sandboxes\n</code></pre>"},{"location":"guides/cleanup-patterns/#batch-cleanup","title":"Batch Cleanup","text":"<pre><code>from aviato import results\n\nsandboxes = [Sandbox.run() for _ in range(5)]\n# ... use sandboxes ...\nresults([sb.stop() for sb in sandboxes])  # Stop all in parallel\n</code></pre>"},{"location":"guides/cleanup-patterns/#orphan-management","title":"Orphan Management","text":""},{"location":"guides/cleanup-patterns/#tagging-for-discovery","title":"Tagging for Discovery","text":"<p>The SDK's automatic cleanup handlers prevent most orphans, but sandboxes can still be left running after forced shutdowns (kill -9), network failures, or when creating sandboxes outside of sessions and context managers. Use tags to make any orphans easily discoverable.</p> <pre><code>from aviato import Sandbox, SandboxDefaults\n\n# Tag at creation time\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"my-project\", \"batch-job-123\"),\n)\n\nwith Sandbox.run(defaults=defaults) as sandbox:\n    result = sandbox.exec([\"echo\", \"hello\"]).result()\n</code></pre> <p>Good tagging practices: - Project or application name (<code>my-project</code>) - Job or run identifier (<code>batch-job-123</code>, <code>run-2024-01-15</code>) - Environment (<code>dev</code>, <code>staging</code>, <code>prod</code>)</p>"},{"location":"guides/cleanup-patterns/#finding-orphaned-sandboxes","title":"Finding Orphaned Sandboxes","text":"<p>Query by tags to find sandboxes from previous runs:</p> <pre><code>from aviato import Sandbox\n\norphans = Sandbox.list(tags=[\"my-project\"]).result()\nfor sandbox in orphans:\n    sandbox.stop().result()\n</code></pre>"},{"location":"guides/cleanup-patterns/#session-adoption","title":"Session Adoption","text":"<p>Bring orphans under session management for automatic cleanup:</p> <pre><code>import aviato\nfrom aviato import SandboxDefaults\n\nwith aviato.Session(SandboxDefaults(container_image=\"python:3.11\")) as session:\n    orphans = session.list(tags=[\"my-project\"]).result()\n    for sandbox in orphans:\n        session.adopt(sandbox)\n# All adopted sandboxes cleaned up with session\n</code></pre>"},{"location":"guides/cleanup-patterns/#deleting-by-id","title":"Deleting by ID","text":"<pre><code>from aviato import Sandbox\n\nSandbox.delete(\"sandbox-abc123\").result()\nSandbox.delete(\"sandbox-abc123\", missing_ok=True)  # Ignore if gone\n</code></pre>"},{"location":"guides/environment-variables/","title":"Environment Variables","text":"<p>This guide covers how to use environment variables in sandboxes.</p> <p>Security Note: Environment variables should not be used for sensitive information like API keys, passwords, or other secrets.</p>"},{"location":"guides/environment-variables/#basic-usage","title":"Basic Usage","text":"<p>Set environment variables when creating a sandbox:</p> <pre><code>from aviato import Sandbox\n\nwith Sandbox.run(\n    environment_variables={\"LOG_LEVEL\": \"info\"},\n) as sandbox:\n    result = sandbox.exec([\n        \"python\",\n        \"-c\",\n        \"import os; print(os.environ.get('LOG_LEVEL'));\",\n    ]).result()\n    print(result.stdout.strip())  # \"info\"\n</code></pre>"},{"location":"guides/environment-variables/#session-level-defaults","title":"Session-Level Defaults","text":"<p>Use sessions to share environment variables across multiple sandboxes:</p> <pre><code>from aviato import SandboxDefaults, Session\n\ndefaults = SandboxDefaults(\n    environment_variables={\n        \"PROJECT_ID\": \"my-project\",\n        \"LOG_LEVEL\": \"info\",\n    },\n)\n\nwith Session(defaults) as session:\n    with session.sandbox() as sb1:\n        result = sb1.exec([\n            \"python\",\n            \"-c\",\n            \"import os; print(os.environ.get('LOG_LEVEL'));\",\n        ]).result()\n        print(result.stdout.strip())  # \"info\"\n\n    # Override LOG_LEVEL and add new variable\n    with session.sandbox(\n        environment_variables={\n            \"LOG_LEVEL\": \"debug\",  # Override session default\n            \"MODEL_NAME\": \"gpt-4\",  # Add new variable\n        }\n    ) as sb2:\n        result = sb2.exec([\n            \"python\",\n            \"-c\",\n            \"import os; \"\n            \"print(os.environ.get('PROJECT_ID')); \"\n            \"print(os.environ.get('LOG_LEVEL')); \"\n            \"print(os.environ.get('MODEL_NAME'));\",\n        ]).result()\n        lines = result.stdout.strip().split(\"\\n\")\n        print(lines)  # [\"my-project\", \"debug\", \"gpt-4\"]\n</code></pre>"},{"location":"guides/environment-variables/#remote-functions","title":"Remote Functions","text":"<p>Environment variables work with remote functions:</p> <pre><code>with Session(defaults) as session:\n    @session.function(environment_variables={\"MODEL_VERSION\": \"v2.0\"})\n    def process(task_id: int) -&gt; dict:\n        import os\n        return {\n            \"task\": task_id,\n            \"project\": os.environ.get(\"PROJECT_ID\"),  # From session defaults\n            \"version\": os.environ.get(\"MODEL_VERSION\"),  # From function decorator\n        }\n\n    result = process.remote(42).result()\n    print(result)  # {\"task\": 42, \"project\": \"my-project\", \"version\": \"v2.0\"}\n</code></pre> <p>\u26a0\ufe0f Caution: Environment variables are passed by reference. Mutations will be reflected in subsequent function calls:</p> <pre><code>env_vars = {\"MODEL_VERSION\": \"v2.0\"}\n\n@session.function(environment_variables=env_vars)\ndef process(task_id: int) -&gt; dict:\n    import os\n    return {\"version\": os.environ.get(\"MODEL_VERSION\")}\n\nresult = process.remote(42).result()  # version: \"v2.0\"\n\nenv_vars[\"MODEL_VERSION\"] = \"v3.0\"  # Mutate the dictionary\n\nresult = process.remote(42).result()  # version: \"v3.0\" (changed)\n</code></pre>"},{"location":"guides/execution/","title":"Command Execution Guide","text":"<p>This guide covers running commands in sandboxes using the <code>exec()</code> method.</p>"},{"location":"guides/execution/#basic-execution","title":"Basic Execution","text":"<p>The <code>exec()</code> method runs commands in a sandbox and returns a <code>Process</code> handle:</p> <pre><code>from aviato import Sandbox\n\nwith Sandbox.run() as sandbox:\n    # Run a command and get the result\n    result = sandbox.exec([\"echo\", \"Hello, World!\"]).result()\n\n    print(result.stdout)      # \"Hello, World!\\n\"\n    print(result.returncode)  # 0\n</code></pre>"},{"location":"guides/execution/#getting-results","title":"Getting Results","text":"<p>The <code>exec()</code> method returns a <code>Process</code> object. Call <code>.result()</code> to block for the output:</p> <pre><code># Returns Process immediately\nprocess = sandbox.exec([\"python\", \"-c\", \"print('hello')\"])\n\n# Block for result\nresult = process.result()\nprint(result.stdout)      # \"hello\\n\"\nprint(result.stderr)      # \"\"\nprint(result.returncode)  # 0\n\n# One-liner pattern\nresult = sandbox.exec([\"ls\", \"-la\"]).result()\n</code></pre>"},{"location":"guides/execution/#streaming-output","title":"Streaming Output","text":"<p>For real-time output, iterate over <code>process.stdout</code> before calling <code>.result()</code>:</p> <pre><code># Returns Process immediately\nprocess = sandbox.exec([\"python\", \"long_script.py\"])\n\n# Stream stdout line by line\nfor line in process.stdout:\n    print(f\"[stdout] {line}\", end=\"\")\n\n# Get final result\nresult = process.result()\nprint(f\"Exit code: {result.returncode}\")\n</code></pre> <p>Use streaming when you need to: - Monitor long-running processes - Process output as it arrives - Implement progress indicators</p>"},{"location":"guides/execution/#working-directory","title":"Working Directory","text":"<p>Set the working directory with <code>cwd</code>:</p> <pre><code>result = sandbox.exec(\n    [\"ls\", \"-la\"],\n    cwd=\"/app/data\",\n).result()\n</code></pre> <p>The path must be absolute.</p>"},{"location":"guides/execution/#timeouts","title":"Timeouts","text":"<p>Set command timeout with <code>timeout_seconds</code>:</p> <pre><code>from aviato import SandboxTimeoutError\n\ntry:\n    result = sandbox.exec(\n        [\"sleep\", \"60\"],\n        timeout_seconds=5.0,\n    ).result()\nexcept SandboxTimeoutError:\n    print(\"Command timed out\")\n</code></pre>"},{"location":"guides/execution/#error-handling-with-check","title":"Error Handling with check","text":"<p>The <code>check</code> parameter controls error behavior for non-zero exit codes:</p>"},{"location":"guides/execution/#checkfalse-default","title":"check=False (Default)","text":"<p>Returns the result regardless of exit code:</p> <pre><code>result = sandbox.exec([\"false\"]).result()\nprint(result.returncode)  # 1 (no exception)\n</code></pre>"},{"location":"guides/execution/#checktrue","title":"check=True","text":"<p>Raises <code>SandboxExecutionError</code> on non-zero exit:</p> <pre><code>from aviato import SandboxExecutionError\n\ntry:\n    result = sandbox.exec(\n        [\"python\", \"-c\", \"raise ValueError('oops')\"],\n        check=True,\n    ).result()\nexcept SandboxExecutionError as e:\n    print(f\"Command failed: {e.exec_result.returncode}\")\n    print(f\"stderr: {e.exec_result.stderr}\")\n</code></pre>"},{"location":"guides/execution/#running-python-code","title":"Running Python Code","text":"<p>Execute Python scripts or one-liners:</p> <pre><code># One-liner\nresult = sandbox.exec(\n    [\"python\", \"-c\", \"import sys; print(sys.version)\"],\n).result()\n\n# Script from string\ncode = '''\nimport json\ndata = {\"result\": 42}\nprint(json.dumps(data))\n'''\nresult = sandbox.exec([\"python\", \"-c\", code]).result()\noutput = json.loads(result.stdout)\n</code></pre>"},{"location":"guides/execution/#sequential-vs-parallel-execution","title":"Sequential vs Parallel Execution","text":""},{"location":"guides/execution/#sequential-order-matters","title":"Sequential (Order Matters)","text":"<pre><code># Dependencies require sequential execution\nsandbox.exec([\"pip\", \"install\", \"requests\"]).result()\nsandbox.exec([\"python\", \"script_using_requests.py\"]).result()\n</code></pre>"},{"location":"guides/execution/#parallel-independent-commands","title":"Parallel (Independent Commands)","text":"<pre><code># Start multiple sandboxes\nsandboxes = [Sandbox.run() for _ in range(3)]\n\n# Start commands on each\nprocesses = [\n    sb.exec([\"python\", \"-c\", f\"print({i})\"])\n    for i, sb in enumerate(sandboxes)\n]\n\n# Collect all results\nresults = [p.result() for p in processes]\nfor r in results:\n    print(r.stdout)\n</code></pre>"},{"location":"guides/execution/#waiting-for-n-of-m-to-complete","title":"Waiting for N of M to Complete","text":"<p>Use <code>aviato.wait()</code> to wait for a subset of processes:</p> <pre><code>import aviato\n\nprocesses = [sb.exec([\"python\", \"task.py\"]) for sb in sandboxes]\n\n# Wait for first 2 to complete\ndone, pending = aviato.wait(processes, num_returns=2)\n\n# Process completed ones immediately\nfor p in done:\n    print(p.result().stdout)\n\n# Wait for remaining\nfor p in pending:\n    print(p.result().stdout)\n</code></pre>"},{"location":"guides/execution/#process-control","title":"Process Control","text":"<p>The <code>Process</code> object provides methods for monitoring and control:</p> <pre><code>process = sandbox.exec([\"python\", \"server.py\"])\n\n# Check if running (non-blocking)\nif process.poll() is None:\n    print(\"Still running\")\n\n# Wait for completion\nexit_code = process.wait()\nprint(f\"Exited with code: {exit_code}\")\n</code></pre>"},{"location":"guides/file-operations/","title":"File Operations Guide","text":"<p>This guide covers reading and writing files in sandboxes.</p>"},{"location":"guides/file-operations/#basic-operations","title":"Basic Operations","text":"<p>File operations return <code>OperationRef</code> objects. Use <code>.result()</code> to block for completion.</p>"},{"location":"guides/file-operations/#writing-files","title":"Writing Files","text":"<pre><code>from aviato import Sandbox\n\nwith Sandbox.run() as sandbox:\n    # Write bytes to a file\n    sandbox.write_file(\"/app/data.txt\", b\"Hello, World!\").result()\n\n    # Write JSON\n    import json\n    config = {\"key\": \"value\", \"count\": 42}\n    sandbox.write_file(\n        \"/app/config.json\",\n        json.dumps(config).encode()\n    ).result()\n</code></pre>"},{"location":"guides/file-operations/#reading-files","title":"Reading Files","text":"<pre><code># Read file contents as bytes\ncontent = sandbox.read_file(\"/app/data.txt\").result()\nprint(content.decode())  # \"Hello, World!\"\n\n# Read JSON\nconfig_bytes = sandbox.read_file(\"/app/config.json\").result()\nconfig = json.loads(config_bytes.decode())\n</code></pre>"},{"location":"guides/file-operations/#parallel-operations","title":"Parallel Operations","text":"<p>File operations return immediately, enabling natural parallelism.</p>"},{"location":"guides/file-operations/#parallel-uploads","title":"Parallel Uploads","text":"<pre><code>from aviato import results\n\n# Start all uploads simultaneously\nwrite_refs = [\n    sandbox.write_file(\"/app/config.json\", config_bytes),\n    sandbox.write_file(\"/app/data.csv\", data_bytes),\n    sandbox.write_file(\"/app/model.pkl\", model_bytes),\n]\n\n# Wait for all to complete\nresults(write_refs)\n</code></pre>"},{"location":"guides/file-operations/#parallel-downloads","title":"Parallel Downloads","text":"<pre><code># Start all downloads simultaneously\nread_refs = [\n    sandbox.read_file(\"/app/output.json\"),\n    sandbox.read_file(\"/app/metrics.json\"),\n    sandbox.read_file(\"/app/logs.txt\"),\n]\n\n# Get all results\noutput, metrics, logs = results(read_refs)\n</code></pre>"},{"location":"guides/file-operations/#upload-process-download-pattern","title":"Upload-Process-Download Pattern","text":"<p>A common workflow: upload input files, run processing, download results.</p> <pre><code>from aviato import Sandbox, results\n\nwith Sandbox.run() as sandbox:\n    # 1. Parallel uploads\n    results([\n        sandbox.write_file(\"/app/config.json\", config_bytes),\n        sandbox.write_file(\"/app/input.csv\", input_bytes),\n    ])\n\n    # 2. Sequential processing\n    sandbox.exec([\"pip\", \"install\", \"-r\", \"requirements.txt\"]).result()\n    sandbox.exec([\"python\", \"/app/process.py\"]).result()\n\n    # 3. Parallel downloads\n    output, metrics = results([\n        sandbox.read_file(\"/app/output.json\"),\n        sandbox.read_file(\"/app/metrics.json\"),\n    ])\n</code></pre>"},{"location":"guides/file-operations/#error-handling","title":"Error Handling","text":""},{"location":"guides/file-operations/#file-not-found","title":"File Not Found","text":"<pre><code>from aviato import SandboxFileError\n\ntry:\n    content = sandbox.read_file(\"/nonexistent/file.txt\").result()\nexcept SandboxFileError as e:\n    print(f\"File error: {e.filepath}\")\n</code></pre>"},{"location":"guides/file-operations/#write-errors","title":"Write Errors","text":"<pre><code>try:\n    sandbox.write_file(\"/readonly/path.txt\", b\"data\").result()\nexcept SandboxFileError as e:\n    print(f\"Cannot write to: {e.filepath}\")\n</code></pre>"},{"location":"guides/file-operations/#binary-files","title":"Binary Files","text":"<p>File operations work with any binary content:</p> <pre><code># Images\nwith open(\"image.png\", \"rb\") as f:\n    sandbox.write_file(\"/app/image.png\", f.read()).result()\n\n# Pickle files (only unpickle data from trusted sources)\nimport pickle\nmodel_bytes = pickle.dumps(my_model)\nsandbox.write_file(\"/app/model.pkl\", model_bytes).result()\n\n# Download and unpickle\nmodel_bytes = sandbox.read_file(\"/app/trained_model.pkl\").result()\ntrained_model = pickle.loads(model_bytes)\n</code></pre>"},{"location":"guides/file-operations/#text-encoding","title":"Text Encoding","text":"<p>Files are transferred as bytes. Handle encoding explicitly:</p> <pre><code># Write text\ntext = \"Hello, Unicode! \"\nsandbox.write_file(\"/app/text.txt\", text.encode(\"utf-8\")).result()\n\n# Read text\ncontent = sandbox.read_file(\"/app/text.txt\").result()\ntext = content.decode(\"utf-8\")\n</code></pre>"},{"location":"guides/file-operations/#large-file-considerations","title":"Large File Considerations","text":"<p>For very large files:</p> <ol> <li>Files are transferred through the API - consider bandwidth</li> <li>Use streaming for large datasets when possible</li> <li>Consider mounting S3/object storage for large data</li> </ol> <pre><code># For large data, use S3 mount instead\nwith Sandbox.run(\n    s3_mount={\n        \"bucket\": \"my-data-bucket\",\n        \"mount_path\": \"/data\",\n        \"read_only\": False,\n    }\n) as sandbox:\n    # Files in /data are backed by S3\n    sandbox.exec([\"ls\", \"/data\"]).result()\n</code></pre>"},{"location":"guides/remote-functions/","title":"Remote Functions Guide","text":"<p>This guide covers the <code>@session.function()</code> decorator for running Python functions in sandboxes.</p>"},{"location":"guides/remote-functions/#overview","title":"Overview","text":"<p>The function decorator API lets you execute Python functions in isolated sandbox containers:</p> <pre><code>import aviato\nfrom aviato import SandboxDefaults\n\nwith aviato.Session(SandboxDefaults(container_image=\"python:3.11\")) as session:\n    @session.function()\n    def compute(x: int, y: int) -&gt; int:\n        return x + y\n\n    # Execute in sandbox\n    result = compute.remote(2, 3).result()\n    print(result)  # 5\n</code></pre>"},{"location":"guides/remote-functions/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/remote-functions/#defining-functions","title":"Defining Functions","text":"<p>Decorate functions with <code>@session.function()</code>:</p> <pre><code>@session.function()\ndef process_data(data: dict) -&gt; dict:\n    # Code runs inside the sandbox\n    import pandas as pd\n    df = pd.DataFrame(data)\n    return {\"mean\": df[\"value\"].mean()}\n</code></pre>"},{"location":"guides/remote-functions/#calling-functions","title":"Calling Functions","text":"<p>Call <code>.remote()</code> on the decorated function to execute in the sandbox:</p> <pre><code># Returns OperationRef immediately\nref = compute.remote(2, 3)\n\n# Block for result\nresult = ref.result()\n\n# One-liner\nresult = compute.remote(2, 3).result()\n</code></pre>"},{"location":"guides/remote-functions/#execution-methods","title":"Execution Methods","text":""},{"location":"guides/remote-functions/#map-parallel-execution","title":"map() - Parallel Execution","text":"<p>Execute across multiple inputs:</p> <pre><code>@session.function()\ndef square(x: int) -&gt; int:\n    return x * x\n\n# Execute for each input\nrefs = square.map((x,) for x in range(10))\n\n# Collect all results\nfrom aviato import results\nall_results = results(refs)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n</code></pre> <p>With tuples for multiple arguments:</p> <pre><code>@session.function()\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\n# Each tuple is unpacked as arguments\nrefs = add.map([(1, 2), (3, 4), (5, 6)])\nall_results = results(refs)  # [3, 7, 11]\n</code></pre>"},{"location":"guides/remote-functions/#local-local-execution","title":"local() - Local Execution","text":"<p>Run locally without a sandbox (useful for testing):</p> <pre><code># No sandbox created - runs in current process\nresult = compute.local(2, 3)\nprint(result)  # 5\n</code></pre>"},{"location":"guides/remote-functions/#serialization-modes","title":"Serialization Modes","text":""},{"location":"guides/remote-functions/#json-default","title":"JSON (Default)","text":"<p>Safe and human-readable, but limited to JSON-serializable types:</p> <pre><code>from aviato import Serialization\n\n@session.function(serialization=Serialization.JSON)\ndef process(data: dict) -&gt; dict:\n    return {\"result\": data[\"value\"] * 2}\n</code></pre>"},{"location":"guides/remote-functions/#pickle","title":"Pickle","text":"<p>Supports complex Python objects:</p> <pre><code>import numpy as np\n\n@session.function(serialization=Serialization.PICKLE)\ndef compute_numpy(arr: np.ndarray) -&gt; np.ndarray:\n    return arr * 2\n\narr = np.array([1, 2, 3])\nresult = compute_numpy.remote(arr).result()  # array([2, 4, 6])\n</code></pre> <p>Use pickle when you need: - NumPy arrays - Pandas DataFrames - Custom class instances - Complex nested objects</p>"},{"location":"guides/remote-functions/#closures-and-globals","title":"Closures and Globals","text":""},{"location":"guides/remote-functions/#closure-variables","title":"Closure Variables","text":"<p>Functions can capture variables from their enclosing scope:</p> <pre><code>multiplier = 10\n\n@session.function()\ndef multiply(x: int) -&gt; int:\n    return x * multiplier  # Captures 'multiplier'\n\nresult = multiply.remote(5).result()  # 50\n</code></pre>"},{"location":"guides/remote-functions/#global-variables","title":"Global Variables","text":"<p>Referenced globals are serialized with the function:</p> <pre><code>CONFIG = {\"threshold\": 0.5}\n\n@session.function()\ndef check_value(x: float) -&gt; bool:\n    return x &gt; CONFIG[\"threshold\"]\n\nresult = check_value.remote(0.7).result()  # True\n</code></pre>"},{"location":"guides/remote-functions/#container-image","title":"Container Image","text":"<p>Override the container image for specific functions:</p> <pre><code>@session.function(container_image=\"pytorch/pytorch:latest\")\ndef train_model(data: dict) -&gt; dict:\n    import torch\n    # GPU training code\n    return {\"loss\": 0.01}\n</code></pre>"},{"location":"guides/remote-functions/#error-handling","title":"Error Handling","text":"<p>Function exceptions propagate to the caller:</p> <pre><code>@session.function()\ndef failing_function() -&gt; None:\n    raise ValueError(\"Something went wrong\")\n\ntry:\n    failing_function.remote().result()\nexcept Exception as e:\n    print(f\"Function failed: {e}\")\n</code></pre>"},{"location":"guides/remote-functions/#when-to-use-functions-vs-sandboxes","title":"When to Use Functions vs Sandboxes","text":"Use Case Recommended API Simple Python computation Function decorator Map/reduce over data Function decorator Interactive workflow, multiple commands Sandbox Streaming output Sandbox File manipulation Sandbox Long-running processes Sandbox"},{"location":"guides/remote-functions/#limitations","title":"Limitations","text":"<p>The function API is intentionally simple. For complex workflows:</p> <ul> <li>Retries/backoff: Implement in calling code</li> <li>Task dependencies/DAGs: Use Airflow, Prefect, etc.</li> <li>Complex scheduling: Use the sandbox API directly</li> </ul>"},{"location":"guides/remote-functions/#complete-example","title":"Complete Example","text":"<pre><code>import aviato\nfrom aviato import SandboxDefaults, Serialization, results\nimport numpy as np\n\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"remote-functions-demo\",),\n)\n\nwith aviato.Session(defaults=defaults) as session:\n    # JSON serialization for simple types\n    @session.function()\n    def square(x: int) -&gt; int:\n        return x * x\n\n    # Pickle for complex types\n    @session.function(serialization=Serialization.PICKLE)\n    def process_array(arr: np.ndarray) -&gt; float:\n        return float(arr.mean())\n\n    # Single execution\n    result = square.remote(7).result()\n    print(f\"7 squared: {result}\")\n\n    # Parallel execution\n    refs = square.map((x,) for x in range(5))\n    all_results = results(refs)\n    print(f\"Squares: {all_results}\")\n\n    # NumPy example\n    arr = np.array([1, 2, 3, 4, 5])\n    mean = process_array.remote(arr).result()\n    print(f\"Array mean: {mean}\")\n</code></pre>"},{"location":"guides/rl-training/","title":"RL Training Guide","text":"<p>Work in Progress</p> <p>This guide is under active development. APIs and examples may change.</p> <p>This guide covers using Aviato sandboxes for reinforcement learning training workflows, focusing on agent training with tool execution.</p>"},{"location":"guides/rl-training/#contents","title":"Contents","text":"<ul> <li>Why Sandboxes for Agent Tool Execution</li> <li>Prerequisites</li> <li>Core Pattern</li> <li>Tagging for Job Metadata</li> <li>Try it: reward_function.py</li> <li>TRL GRPOTrainer Integration</li> <li>Try it: trl_grpo_integration.py</li> <li>Error Handling in Agent Episodes</li> <li>W&amp;B Metrics Integration</li> <li>Monitoring and Debugging</li> <li>Multi-step Rollouts with ART</li> </ul>"},{"location":"guides/rl-training/#why-sandboxes-for-agent-tool-execution","title":"Why Sandboxes for Agent Tool Execution","text":"<p>Training code agents with reinforcement learning requires executing tool calls (bash commands, file operations) in isolated environments. Running untrusted commands from a model is risky: the code might modify the host filesystem, make network requests, or produce non-deterministic results. Sandboxes solve this by providing isolated, ephemeral environments where tool calls execute without affecting the host or other rollouts.</p> <p>In an agent training loop, the model generates actions (tool calls), the sandbox executes them, and observations flow back to the model. The sandbox persists across multiple tool calls within an episode, maintaining state as the agent works through a task. Reward is computed based on the final sandbox state (e.g., tests passing) or trajectory quality.</p> <p>Aviato sandboxes start quickly and handle concurrent executions. The tagging and listing APIs help with cleanup and monitoring when you're running thousands of episodes.</p>"},{"location":"guides/rl-training/#prerequisites","title":"Prerequisites","text":"<p>Set your API key:</p> <pre><code>export AVIATO_API_KEY=\"your-api-key\"\n</code></pre> <p>Install aviato from source (from repo root):</p> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"guides/rl-training/#core-pattern","title":"Core Pattern","text":"<p>The basic setup: an agent loop runs on your training infrastructure, and tool calls execute in a sandbox.</p> <pre><code>import aviato\nfrom aviato import Sandbox\n\ndef run_agent_episode(model, task: dict, sandbox: Sandbox) -&gt; tuple[list, float]:\n    \"\"\"Run one agent episode, returning trajectory and reward.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": task[\"prompt\"]}]\n\n    for step in range(task.get(\"max_steps\", 10)):\n        # Model generates next action\n        response = model.generate(messages)\n        messages.append({\"role\": \"assistant\", \"content\": response})\n\n        tool_calls = parse_tool_calls(response)\n        if not tool_calls:\n            break\n\n        # Execute tool calls in sandbox\n        for tool in tool_calls:\n            if tool.name == \"bash\":\n                result = sandbox.exec(\n                    [\"bash\", \"-c\", tool.command],\n                    timeout_seconds=30.0,\n                ).result()\n                observation = f\"exit={result.returncode}\\n{result.stdout}{result.stderr}\"\n            elif tool.name == \"read_file\":\n                content = sandbox.read_file(tool.path).result()\n                observation = content.decode()\n            elif tool.name == \"write_file\":\n                sandbox.write_file(tool.path, tool.content.encode()).result()\n                observation = \"File written successfully\"\n\n            messages.append({\"role\": \"tool\", \"name\": tool.name, \"content\": observation})\n\n    # Compute reward from final sandbox state\n    test_result = sandbox.exec(task[\"test_command\"]).result()\n    reward = 1.0 if test_result.returncode == 0 else 0.0\n\n    return messages, reward\n</code></pre> <p>The sandbox persists across tool calls within an episode, so file changes accumulate as the agent works.</p>"},{"location":"guides/rl-training/#training-step-with-parallel-episodes","title":"Training step with parallel episodes","text":"<p>Process a batch of tasks with one sandbox per episode:</p> <pre><code>def training_step(model, batch: list[dict], session) -&gt; list[float]:\n    \"\"\"Run agent episodes for a batch of tasks.\"\"\"\n\n    # Create sandboxes in parallel\n    sandboxes = [session.sandbox() for _ in batch]\n\n    trajectories = []\n    rewards = []\n\n    for task, sandbox in zip(batch, sandboxes):\n        trajectory, reward = run_agent_episode(model, task, sandbox)\n        trajectories.append(trajectory)\n        rewards.append(reward)\n        sandbox.stop()  # Non-blocking cleanup\n\n    # trajectories and rewards go to policy update\n    return rewards\n</code></pre>"},{"location":"guides/rl-training/#tagging-for-job-metadata","title":"Tagging for Job Metadata","text":"<p>Tags let you filter and find sandboxes created by your training jobs. Include metadata that helps identify sandboxes when debugging or cleaning up:</p> <pre><code>import os\nfrom aviato import SandboxDefaults\n\ndef make_defaults(model_name: str) -&gt; SandboxDefaults:\n    return SandboxDefaults(\n        container_image=\"python:3.11\",\n        tags=(\n            f\"wandb-run:{os.environ.get('WANDB_RUN_ID', 'local')}\",\n            f\"slurm-job:{os.environ.get('SLURM_JOB_ID', 'interactive')}\",\n            f\"model:{model_name}\",\n            \"rl-training\",\n        ),\n    )\n</code></pre> <p>Useful metadata to include in tags:</p> Tag Pattern Purpose <code>wandb-run:{id}</code> W&amp;B run ID (from <code>WANDB_RUN_ID</code> env var) for filtering by training run <code>slurm-job:{id}</code> Slurm job ID (from <code>SLURM_JOB_ID</code> env var) for cluster job tracking <code>model:{name}</code> Model name or checkpoint for multi-model experiments <code>env:{name}</code> Environment (dev, staging, prod) for resource management <p>Sandbox tags become Kubernetes pod labels, which the CoreWeave observability platform uses for filtering and dashboards.</p>"},{"location":"guides/rl-training/#try-it-reward_functionpy","title":"Try it: reward_function.py","text":"<p>The simplest integration: compute code execution rewards with parallel sandbox execution.</p> <p>What it does: - Executes a set of toy code completions (arithmetic, string operations, syntax errors, runtime errors) - Creates one sandbox per completion for isolation - Computes binary rewards: 1.0 for successful execution, 0.0 for failure - Shows progress as results arrive (faster executions complete first)</p> <p>How it uses Aviato:</p> <p>The example uses <code>aviato.wait()</code> to process results as they complete:</p> <pre><code># Create sandboxes and execute all completions in parallel\nprocesses = [\n    session.sandbox().exec(\n        [\"python\", \"-c\", code],\n        timeout_seconds=EXECUTION_TIMEOUT_SECONDS,\n    )\n    for code in completions\n]\n\n# Collect results as they complete\nwhile pending:\n    [process], pending = aviato.wait(pending, num_returns=1)\n    result = process.result()\n    reward = 1.0 if result.returncode == 0 else 0.0\n</code></pre> <p>Run it:</p> <pre><code>uv run examples/rl_training/reward_function.py\n</code></pre> <p>No additional dependencies required. No GPU needed.</p> <p>Expected output:</p> <p>Results arrive as executions complete, so faster problems finish first:</p> <pre><code>RL Training Reward Function Example (job: 4768f471)\n============================================================\n\nEvaluating 5 completions...\n\nProgress (results arrive as executions complete):\n------------------------------------------------------------\n  [1/5] Problem 0 (slow-sum): PASS\n  [2/5] Problem 1 (string-ops): PASS\n  [3/5] Problem 2 (delayed-error): FAIL\n  [4/5] Problem 3 (syntax-error): FAIL\n  [5/5] Problem 4 (slow-list): PASS\n------------------------------------------------------------\n\nFinal summary (original order):\n------------------------------------------------------------\n  Problem 0 (slow-sum): reward=1.0 [PASS] OK\n  Problem 1 (string-ops): reward=1.0 [PASS] OK\n  Problem 2 (delayed-error): reward=0.0 [FAIL] OK\n  Problem 3 (syntax-error): reward=0.0 [FAIL] OK\n  Problem 4 (slow-list): reward=1.0 [PASS] OK\n------------------------------------------------------------\nTotal reward: 3.0/5\nPass rate: 3/5 (60%)\n</code></pre>"},{"location":"guides/rl-training/#trl-grpotrainer-integration","title":"TRL GRPOTrainer Integration","text":"<p>TRL uses a reward function interface where completions map directly to rewards. The agent generates a completion, and the reward function executes it in a sandbox.</p> <p>The standard pattern uses <code>&lt;answer&gt;</code> XML tags for code extraction (matching the format used in GRPO math examples with <code>\\boxed{}</code>):</p> <pre><code>import aviato\nfrom aviato import SandboxDefaults\n\nsession = aviato.Session(defaults=SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"trl-grpo\",),\n))\n\ndef extract_xml_answer(text: str) -&gt; str:\n    if \"&lt;answer&gt;\" not in text:\n        return \"\"\n    return text.split(\"&lt;answer&gt;\")[-1].split(\"&lt;/answer&gt;\")[0].strip()\n\ndef reward_fn(completions: list[str], **kwargs) -&gt; list[float]:\n    codes = [extract_xml_answer(c) for c in completions]\n    code_indices = [(i, code) for i, code in enumerate(codes) if code]\n\n    processes = [\n        (i, session.sandbox().exec(\n            [\"python\", \"-c\", code],\n            timeout_seconds=30.0,\n        ))\n        for i, code in code_indices\n    ]\n\n    rewards = [0.0] * len(codes)\n    for i, process in processes:\n        try:\n            rewards[i] = 1.0 if process.result().returncode == 0 else 0.0\n        except Exception:\n            pass\n\n    return rewards\n</code></pre> <p>This pattern works for training models to generate correct code in a single turn.</p>"},{"location":"guides/rl-training/#try-it-trl_grpo_integrationpy","title":"Try it: trl_grpo_integration.py","text":"<p>Aviato sandboxes with TRL's GRPOTrainer for code execution rewards.</p> <p>What it does: - Loads a small model (<code>Qwen/Qwen2.5-0.5B-Instruct</code>) - Creates a toy dataset of simple coding problems - Trains the model using GRPO with sandbox-based reward computation - Runs 10 training steps to demonstrate the integration</p> <p>How it uses Aviato:</p> <p>The reward function extracts code from <code>&lt;answer&gt;</code> tags (the standard GRPO pattern), creates sandboxes in parallel through a Session, executes each completion, and returns binary rewards:</p> <pre><code>def extract_xml_answer(text: str) -&gt; str:\n    \"\"\"Extract answer from XML-style &lt;answer&gt; tags.\"\"\"\n    if \"&lt;answer&gt;\" not in text:\n        return \"\"\n    answer = text.split(\"&lt;answer&gt;\")[-1]\n    answer = answer.split(\"&lt;/answer&gt;\")[0]\n    return answer.strip()\n\ndef code_execution_reward(completions: list[str], **kwargs) -&gt; list[float]:\n    codes = [extract_xml_answer(c) for c in completions]\n\n    # Create sandboxes and execute non-empty code in parallel\n    processes = [\n        (i, session.sandbox().exec(\n            [\"python\", \"-c\", code],\n            timeout_seconds=EXECUTION_TIMEOUT_SECONDS,\n        ))\n        for i, code in enumerate(codes) if code\n    ]\n\n    # Collect rewards, defaulting to 0.0\n    rewards = [0.0] * len(codes)\n    for i, process in processes:\n        try:\n            result = process.result()\n            rewards[i] = 1.0 if result.returncode == 0 else 0.0\n        except Exception:\n            rewards[i] = 0.0\n\n    return rewards\n</code></pre> <p>The prompts use a system message instructing the model to format code with <code>&lt;answer&gt;</code> tags:</p> <pre><code>SYSTEM_PROMPT = \"\"\"You solve coding problems by writing Python code.\nPut your code inside &lt;answer&gt; tags like this: &lt;answer&gt;print(\"hello\")&lt;/answer&gt;\nOnly include the code, no explanations.\"\"\"\n</code></pre> <p>The Session tracks sandboxes and cleans them up when it closes.</p> <p>Run it:</p> <pre><code>uv pip install trl==0.27.1 transformers==5.0.0 datasets==4.5.0 torch==2.10.0\nuv run examples/rl_training/trl_grpo_integration.py\n</code></pre> <p>GPU is recommended for reasonable performance. Without one, training works but is slow.</p> <p>Expected output:</p> <pre><code>TRL GRPO Integration Example (job: def67890)\n============================================================\n\nLoading model: Qwen/Qwen2.5-0.5B-Instruct\nCreating toy dataset...\nDataset size: 5 problems\n\nSetting up GRPOTrainer...\n\nStarting training (10 steps)...\n------------------------------------------------------------\n  [Aviato] Reward call 1: 2 sandboxes, 0/2 passed\n  [Aviato] Reward call 2: 1 sandboxes, 0/1 passed, 1 skipped (no code)\n  [Aviato] Reward call 3: 0 sandboxes, 0/0 passed, 2 skipped (no code)\n  [Aviato] Reward call 4: 2 sandboxes, 0/2 passed\n  ...\n  [Aviato] Reward call 8: 2 sandboxes, 1/2 passed\n  [Aviato] Reward call 9: 2 sandboxes, 1/2 passed\n  [Aviato] Reward call 10: 1 sandboxes, 0/1 passed, 1 skipped (no code)\n[training logs]\n------------------------------------------------------------\n\nTraining completed successfully!\n</code></pre> <p>Understanding the output:</p> <p>The number of sandboxes varies per step because we only create sandboxes when <code>extract_code_block()</code> finds extractable Python code in the model's completion. When the model generates text without recognizable code (no markdown fences like <code>```python</code>, no <code>&lt;code&gt;</code> tags), that completion is skipped and receives a reward of 0.0.</p> <ul> <li><code>2 sandboxes, 0/2 passed</code> - Model generated 2 code blocks, both failed execution</li> <li><code>1 sandboxes, 0/1 passed, 1 skipped (no code)</code> - Model generated 1 code block (failed) and 1 text-only completion</li> <li><code>0 sandboxes, 0/0 passed, 2 skipped (no code)</code> - Model generated no extractable code in either completion</li> </ul> <p>Expected with a small, untrained model. As training progresses, you should see fewer skipped completions and more passes.</p>"},{"location":"guides/rl-training/#error-handling-in-agent-episodes","title":"Error Handling in Agent Episodes","text":"<p>Sandbox operations can fail (timeouts, missing files, sandbox termination). Return observations that help the agent understand what went wrong:</p> <pre><code>from aviato import SandboxTimeoutError, SandboxFileError\n\ndef execute_tool(sandbox, tool) -&gt; str:\n    \"\"\"Execute a tool call, returning an observation string.\"\"\"\n    try:\n        if tool.name == \"bash\":\n            result = sandbox.exec(\n                [\"bash\", \"-c\", tool.command],\n                timeout_seconds=30.0,\n            ).result()\n            return f\"exit={result.returncode}\\n{result.stdout}{result.stderr}\"\n        elif tool.name == \"read_file\":\n            content = sandbox.read_file(tool.path).result()\n            return content.decode()\n        elif tool.name == \"write_file\":\n            sandbox.write_file(tool.path, tool.content.encode()).result()\n            return \"File written successfully\"\n    except SandboxTimeoutError:\n        return \"Error: command timed out after 30 seconds\"\n    except SandboxFileError as e:\n        return f\"Error: {e}\"\n    except Exception as e:\n        return f\"Error: {type(e).__name__}: {e}\"\n</code></pre> <p>For reward computation, catch exceptions and return a fallback reward instead of propagating to the training loop.</p>"},{"location":"guides/rl-training/#wb-metrics-integration","title":"W&amp;B Metrics Integration","text":"<p>When using W&amp;B (Weights &amp; Biases) for training, aviato Sessions automatically log sandbox usage metrics to your active wandb run. You can see how your training uses sandboxes without writing extra instrumentation code.</p>"},{"location":"guides/rl-training/#auto-detection","title":"Auto-detection","text":"<p>If <code>WANDB_API_KEY</code> is set and a wandb run is active (<code>wandb.run</code> exists), metrics logging is enabled automatically:</p> <pre><code>import wandb\nfrom aviato import Session, SandboxDefaults\n\nwandb.init(project=\"my-rl-training\")\n\n# Metrics logging enabled automatically\nwith Session(defaults) as session:\n    for step in range(num_steps):\n        sandbox = session.sandbox()\n        # Exec results are automatically tracked - no manual calls needed\n        result = sandbox.exec([\"python\", \"-c\", code]).result()\n        # Log metrics at this training step for correlation\n        session.log_metrics(step=step)\n# Final metrics logged on session close\n</code></pre>"},{"location":"guides/rl-training/#explicit-control","title":"Explicit Control","text":"<p>Control metrics reporting with the <code>report_to</code> parameter:</p> <pre><code># Explicit opt-in (reports even without active wandb run)\nsession = Session(defaults, report_to=[\"wandb\"])\n\n# Disable reporting (even if wandb run exists)\nsession = Session(defaults, report_to=[])\n\n# Auto-detect (default behavior)\nsession = Session(defaults, report_to=None)\n</code></pre>"},{"location":"guides/rl-training/#metrics","title":"Metrics","text":"<p>Execution metrics are tracked automatically when <code>exec()</code> completes:</p> Metric Description <code>aviato/sandboxes_created</code> Total sandboxes created via session <code>aviato/executions</code> Total exec() calls <code>aviato/exec_successes</code> Successful executions (returncode=0) <code>aviato/exec_failures</code> Failed executions (returncode!=0) <code>aviato/exec_errors</code> Errors (timeouts, transport failures) <code>aviato/success_rate</code> Fraction of exec() with returncode=0 <code>aviato/error_rate</code> Fraction of exec() that errored <code>aviato/avg_execs_per_sandbox</code> Average exec() calls per sandbox <code>aviato/min_execs_per_sandbox</code> Minimum exec() calls in any sandbox <code>aviato/max_execs_per_sandbox</code> Maximum exec() calls in any sandbox <p>Tracking is automatic: just call <code>exec()</code> on any sandbox associated with a session. Call <code>session.log_metrics(step=N)</code> to log at specific training steps:</p> <pre><code>def training_step(session, model, batch, step: int) -&gt; list[float]:\n    rewards = []\n    for task in batch:\n        sandbox = session.sandbox()\n        # Metrics tracked automatically on exec() completion\n        result = sandbox.exec([\"python\", \"-c\", task[\"code\"]]).result()\n        reward = 1.0 if result.returncode == 0 else 0.0\n        rewards.append(reward)\n        sandbox.stop()\n\n    # Log metrics at this training step for correlation\n    session.log_metrics(step=step)\n    return rewards\n</code></pre> <p>You can also access per-sandbox statistics via the <code>execution_stats</code> property:</p> <pre><code>sandbox = session.sandbox()\nresult = sandbox.exec([\"echo\", \"hello\"]).result()\nprint(sandbox.execution_stats)  # {\"total\": 1, \"successes\": 1, \"failures\": 0, \"errors\": 0}\n</code></pre>"},{"location":"guides/rl-training/#per-sandbox-exec-metrics","title":"Per-Sandbox Exec Metrics","text":"<p>When using W&amp;B integration with Sessions, the following per-sandbox metrics are automatically tracked:</p> Metric Description <code>aviato/avg_execs_per_sandbox</code> Average exec() calls per sandbox (useful for \"tool calls per rollout\") <code>aviato/min_execs_per_sandbox</code> Minimum exec() calls in any sandbox <code>aviato/max_execs_per_sandbox</code> Maximum exec() calls in any sandbox <p>These metrics help understand agent behavior during RL training:</p> <ul> <li>High avg_execs_per_sandbox may indicate verbose agents that make many tool calls per episode</li> <li>Large variance (max-min) may indicate inconsistent rollout behavior across episodes</li> <li>Trends over training steps show how agent behavior evolves as the policy improves</li> </ul> <p>Example dashboard usage:</p> <ul> <li>Plot <code>avg_execs_per_sandbox</code> vs training step to see tool usage trends over training</li> <li>Alert if <code>max_execs_per_sandbox</code> exceeds a threshold (runaway agent making excessive tool calls)</li> <li>Compare min/max spread to detect episodes where agents get stuck in loops vs complete quickly</li> </ul> <p>By default, <code>log_metrics()</code> resets the counters after logging. Set <code>reset=False</code> to keep accumulating:</p> <pre><code>session.log_metrics(step=step, reset=False)  # Keep accumulating\n</code></pre> <p>Metrics are also logged automatically when the session closes, so you get final summary metrics even without explicit logging.</p>"},{"location":"guides/rl-training/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"guides/rl-training/#counting-active-sandboxes","title":"Counting Active Sandboxes","text":"<p>Monitor sandbox usage during training:</p> <pre><code>from aviato import Sandbox, SandboxStatus\n\ndef count_active_sandboxes(run_id: str) -&gt; dict:\n    sandboxes = Sandbox.list(\n        tags=[f\"wandb-run:{run_id}\"],\n        status=[SandboxStatus.RUNNING, SandboxStatus.PENDING],\n    ).result()\n\n    return {\n        \"running\": sum(1 for s in sandboxes if s.status == SandboxStatus.RUNNING),\n        \"pending\": sum(1 for s in sandboxes if s.status == SandboxStatus.PENDING),\n        \"total\": len(sandboxes),\n    }\n</code></pre>"},{"location":"guides/rl-training/#logging-execution-details","title":"Logging Execution Details","text":"<p>Capture execution details for debugging reward computation:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n# Assumes `session` is created at module level\n\ndef logged_reward(completion: str, step: int) -&gt; float:\n    code = extract_code(completion)\n\n    sandbox = session.sandbox()\n    sandbox.wait()\n    result = sandbox.exec(\n        [\"python\", \"-c\", code],\n        timeout_seconds=30.0,\n    ).result()\n\n    logger.debug(\n        \"Reward computation\",\n        extra={\n            \"step\": step,\n            \"sandbox_id\": sandbox.sandbox_id,\n            \"returncode\": result.returncode,\n            \"stdout_len\": len(result.stdout),\n            \"stderr_len\": len(result.stderr),\n        },\n    )\n\n    sandbox.stop()\n    return 1.0 if result.returncode == 0 else 0.0\n</code></pre>"},{"location":"guides/rl-training/#multi-step-rollouts-with-art","title":"Multi-step Rollouts with ART","text":"<p>The TRL example above uses sandboxes for single-shot execution: one sandbox per completion, execute once, return a reward. This works for training models to generate correct code in one attempt.</p> <p>Stateful multi-step rollouts are different: the agent takes multiple actions within a single sandbox, and the sandbox maintains state between actions. The agent can write a file, run it, see the error, edit the file, and try again - all within the same sandbox.</p> <p>The <code>examples/rl_training/art/</code> directory demonstrates this pattern on the MBPP benchmark. When a solution fails, the agent receives error feedback and can iterate on its approach.</p>"},{"location":"guides/rl-training/#overview","title":"Overview","text":"<p>ART (Agent Reinforcement Trainer) is an open-source RL framework by OpenPipe for training multi-step agents using GRPO. This example integrates Aviato sandboxes with ART:</p> <ul> <li>Uses the <code>art</code> package (<code>openpipe-art</code>) for trajectory collection and training</li> <li>Supports two backends: <code>LocalBackend</code> (requires GPU) or <code>TinkerBackend</code> (no GPU)</li> <li>Executes code via tool calling in Aviato sandboxes</li> <li>Computes binary rewards based on MBPP test case results</li> </ul>"},{"location":"guides/rl-training/#training-approach-grpo-with-distillation","title":"Training Approach: GRPO with Distillation","text":"<p>This example uses distillation with reinforcement learning: a stronger model generates demonstrations, and a smaller model learns to replicate the successful ones.</p> <p>Two models are involved:</p> <ol> <li> <p>Inference model (<code>--model</code>, default: <code>gpt-5.1-codex-mini</code>): Generates trajectories during rollouts. This model makes tool calls, sees sandbox results, iterates on errors, and submits solutions. It does not get trained.</p> </li> <li> <p>Base model (<code>--base-model</code>, default: <code>Qwen/Qwen3-8B</code>): The model being trained. It receives the trajectories generated by the inference model and learns from them via GRPO (Group Relative Policy Optimization).</p> </li> </ol> <p>How it works:</p> <ol> <li>The inference model generates multiple trajectories per problem, each with tool calls executed in Aviato sandboxes</li> <li>Each trajectory receives a binary reward: 1.0 if tests pass, 0.0 otherwise</li> <li>Trajectories for the same problem form a group - GRPO compares trajectories within each group</li> <li>The base model (Qwen3-8B) is trained to prefer higher-reward trajectories over lower-reward ones</li> </ol> <p>After training, you deploy Qwen3-8B with the same tool definitions. It will have learned to make similar tool calls by imitating the successful trajectories from the inference model.</p>"},{"location":"guides/rl-training/#prerequisites_1","title":"Prerequisites","text":"Mode Requirements Dry run CPU only TinkerBackend CPU only (training via API) LocalBackend GPU required <p>Environment variables:</p> <pre><code>export AVIATO_API_KEY=\"your-aviato-key\"\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ART_TINKER_API_KEY=\"your-tinker-key\"  # required for --backend=tinker\nexport WANDB_API_KEY=\"your-wandb-key\"        # optional, for logging\n</code></pre>"},{"location":"guides/rl-training/#installation","title":"Installation","text":"<pre><code>uv pip install -r examples/rl_training/art/requirements.txt\n</code></pre> <p>This installs:</p> <pre><code>openpipe-art==0.5.7       # ART framework\nopenai==2.15.0            # LLM inference\ndatasets==4.5.0           # MBPP loading\nwandb==0.24.0             # Optional logging\n</code></pre> <p>For LocalBackend with GPU support, also install:</p> <pre><code>uv pip install \"openpipe-art[backend]==0.5.7\"\n</code></pre>"},{"location":"guides/rl-training/#running-the-example","title":"Running the Example","text":"<pre><code># Dry run - validate setup without training\nuv run examples/rl_training/art/train.py --dry-run\n\n# Train with TinkerBackend (no GPU required)\nuv run examples/rl_training/art/train.py --backend tinker --num-problems 10\n\n# Train with LocalBackend (requires GPU)\nuv run examples/rl_training/art/train.py --backend local --num-problems 10\n</code></pre> <p>Expected output:</p> <pre><code>ART Training with Aviato Sandboxes\n========================================\nBackend: tinker\nModel: gpt-5.1-codex-mini\nBase model: Qwen/Qwen3-8B-Instruct\nProblems: 10\nSteps: 5\nTrajectories per problem: 2\nProject: aviato-mbpp\nRun name: train-001\n\nLoading MBPP problems...\nLoaded 10 problems\n\nCreating tinker backend...\nCreating trainable model...\nRegistering model with backend...\n\nStarting training...\n\n=== Step 1 ===\nCollecting trajectories for 10 problems...\nstep 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:45&lt;00:00]\nCollected 20 trajectories, avg reward: 0.35\nTraining...\nTraining complete: step=1, metrics={'loss': 0.42}\n...\n</code></pre>"},{"location":"guides/rl-training/#configuration-options","title":"Configuration Options","text":"Flag Default Description <code>--backend</code> <code>local</code> Training backend: <code>local</code> (GPU) or <code>tinker</code> (no GPU) <code>--model</code> <code>gpt-5.1-codex-mini</code> Model for inference <code>--base-model</code> <code>Qwen/Qwen3-8B-Instruct</code> Base model for training <code>--num-problems</code> <code>10</code> Number of MBPP problems <code>--num-steps</code> <code>5</code> Training steps <code>--trajectories-per-problem</code> <code>2</code> Trajectories collected per problem per step <code>--base-url</code> <code>None</code> OpenAI-compatible API base URL <code>--project</code> <code>aviato-mbpp</code> W&amp;B project name <code>--run-name</code> <code>train-001</code> Training run name <code>--learning-rate</code> <code>1e-5</code> Learning rate <code>--dry-run</code> <code>false</code> Validate setup without training"},{"location":"guides/rl-training/#architecture","title":"Architecture","text":"<pre><code>art/\n\u251c\u2500\u2500 train.py         # Training loop, CLI, and ART backend setup\n\u251c\u2500\u2500 rollout.py       # Multi-step sandbox execution, builds Trajectory\n\u251c\u2500\u2500 tools.py         # Tool schemas for execute_code and submit_solution\n\u2514\u2500\u2500 __init__.py\n</code></pre> <p>Key ART imports:</p> <pre><code>import art\nfrom art.local import LocalBackend\nfrom art.tinker import TinkerBackend\n\n# Create trainable model\nmodel = art.TrainableModel(\n    name=\"train-001\",\n    project=\"aviato-mbpp\",\n    base_model=\"Qwen/Qwen3-8B-Instruct\",\n)\n\n# Collect trajectories\ngroups = await art.gather_trajectory_groups(\n    (collect_trajectories(problem) for problem in problems),\n    pbar_desc=\"collecting\",\n)\n\n# Train\nresult = await backend.train(model, groups, learning_rate=1e-5)\n</code></pre> <p>Rollout returns <code>art.Trajectory</code>:</p> <pre><code>from openai.types.chat import ChatCompletionToolParam\n\ntrajectory = art.Trajectory(\n    messages_and_choices=messages_and_choices,  # Conversation history\n    tools=ROLLOUT_TOOLS,                        # Tool definitions\n    reward=1.0 if passed else 0.0,              # Binary reward\n    metadata={\"task_id\": problem.task_id},\n)\nreturn trajectory.finish()\n</code></pre> <p>Tool-calling pattern:</p> <p>The rollout uses OpenAI-compatible tool calling with two tools: - <code>execute_code</code>: Test code in sandbox, returns stdout/stderr - <code>submit_solution</code>: Final submission, runs all test cases</p> <pre><code>ROLLOUT_TOOLS: list[ChatCompletionToolParam] = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"execute_code\",\n            \"description\": \"Execute Python code in an isolated sandbox...\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"code\": {\"type\": \"string\"}},\n                \"required\": [\"code\"],\n            },\n        },\n    },\n    # submit_solution tool...\n]\n</code></pre> <p>Sandboxes are tagged for tracking:</p> <pre><code>sandbox_defaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"art-training\", args.project, args.run_name),\n)\n</code></pre>"},{"location":"guides/rl-training/#data-flow","title":"Data Flow","text":"<p>The training pipeline has several components:</p> <ol> <li> <p>Local machine: The training script (<code>train.py</code>) runs on your machine or training server. It loads problems from the MBPP dataset and orchestrates the training loop.</p> </li> <li> <p>OpenAI API (inference): During trajectory collection, the rollout code calls the OpenAI API (or compatible endpoint) to generate model responses. The model receives tool definitions and returns tool calls that the rollout executes.</p> </li> <li> <p>Aviato sandboxes (code execution): Each rollout uses a single Aviato sandbox that persists across all tool calls. When the model calls <code>execute_code</code> or <code>submit_solution</code>, the code runs in that sandbox. This means file changes and state accumulate as the agent iterates - it can write a file, run it, see an error, and fix it. The sandbox provides isolation so untrusted model-generated code cannot affect the host. Results (stdout, stderr, exit code) flow back to the rollout.</p> </li> <li> <p>Trajectory collection: The rollout accumulates the conversation history (messages and tool results) along with the final reward into an <code>art.Trajectory</code> object. Multiple trajectories for the same problem form an <code>art.TrajectoryGroup</code>.</p> </li> <li> <p>Training backend: The collected trajectory groups are sent to the training backend. With <code>LocalBackend</code>, training happens on your local GPU. With <code>TinkerBackend</code>, trajectories are uploaded to Thinking Machines's Tinker service which handles training remotely - no local GPU required.</p> </li> </ol>"},{"location":"guides/sandbox-configuration/","title":"Sandbox Configuration Guide","text":"<p>This guide covers sandbox configuration options: resources, mounted files, ports, and timeouts.</p>"},{"location":"guides/sandbox-configuration/#overview","title":"Overview","text":"<p>Sandbox configuration can be set in three places:</p> <ol> <li>SandboxDefaults - Shared defaults for all sandboxes in a session</li> <li>Sandbox.run() kwargs - Per-sandbox overrides</li> <li>@session.function() kwargs - Function-specific configuration</li> </ol> <pre><code>from aviato import Sandbox, SandboxDefaults, Session\n\n# Via SandboxDefaults\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    max_lifetime_seconds=3600,\n)\n\n# Via Sandbox.run() kwargs\nsandbox = Sandbox.run(\n    defaults=defaults,\n    resources={\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n)\n\n# Via @session.function() kwargs\nwith Session(defaults) as session:\n    @session.function(resources={\"cpu\": \"1000m\"})\n    def compute(x: int) -&gt; int:\n        return x * 2\n</code></pre>"},{"location":"guides/sandbox-configuration/#resources","title":"Resources","text":"<p>Request CPU, memory, and GPU resources for sandboxes:</p> <pre><code>sandbox = Sandbox.run(\n    resources={\n        \"cpu\": \"500m\",       # 500 millicores (0.5 CPU)\n        \"memory\": \"512Mi\",   # 512 MiB\n    },\n)\n</code></pre>"},{"location":"guides/sandbox-configuration/#cpu","title":"CPU","text":"<p>CPU is specified in millicores or whole cores:</p> Value Meaning <code>\"100m\"</code> 100 millicores (0.1 CPU) <code>\"500m\"</code> 500 millicores (0.5 CPU) <code>\"1000m\"</code> or <code>\"1\"</code> 1 full CPU core <code>\"2000m\"</code> or <code>\"2\"</code> 2 CPU cores"},{"location":"guides/sandbox-configuration/#memory","title":"Memory","text":"<p>Memory uses standard Kubernetes units:</p> Value Meaning <code>\"128Mi\"</code> 128 mebibytes <code>\"512Mi\"</code> 512 mebibytes <code>\"1Gi\"</code> 1 gibibyte <code>\"4Gi\"</code> 4 gibibytes"},{"location":"guides/sandbox-configuration/#gpu","title":"GPU","text":"<p>Request GPU resources:</p> <pre><code>sandbox = Sandbox.run(\n    resources={\n        \"cpu\": \"4000m\",\n        \"memory\": \"16Gi\",\n        \"gpu\": \"1\",          # Request 1 GPU\n    },\n)\n</code></pre>"},{"location":"guides/sandbox-configuration/#mounted-files","title":"Mounted Files","text":"<p>Mount files into the sandbox at startup:</p> <pre><code>sandbox = Sandbox.run(\n    mounted_files=[\n        {\n            \"path\": \"/app/config.json\",\n            \"content\": '{\"debug\": true}',\n        },\n        {\n            \"path\": \"/app/script.py\",\n            \"content\": \"print('hello')\",\n        },\n    ],\n)\n\n# Files are available immediately\nresult = sandbox.exec([\"python\", \"/app/script.py\"]).result()\n</code></pre>"},{"location":"guides/sandbox-configuration/#mount-options","title":"Mount Options","text":"Field Type Description <code>path</code> str Absolute path in sandbox <code>content</code> str File content (text) <p>Mounted files are read-only. Use <code>write_file()</code> for files that need modification.</p>"},{"location":"guides/sandbox-configuration/#ports","title":"Ports","text":"<p>Expose ports from the sandbox:</p> <pre><code>sandbox = Sandbox.run(\n    \"python\", \"-m\", \"http.server\", \"8080\",\n    ports=[\n        {\"container_port\": 8080},\n    ],\n)\n</code></pre>"},{"location":"guides/sandbox-configuration/#port-configuration","title":"Port Configuration","text":"Field Type Description <code>container_port</code> int Port inside the sandbox"},{"location":"guides/sandbox-configuration/#service","title":"Service","text":"<p>Configure network service options:</p> <pre><code>sandbox = Sandbox.run(\n    service={\n        \"name\": \"my-service\",\n    },\n)\n</code></pre>"},{"location":"guides/sandbox-configuration/#timeouts","title":"Timeouts","text":""},{"location":"guides/sandbox-configuration/#timeout_seconds","title":"timeout_seconds","text":"<p>Per-command timeout:</p> <pre><code># Per-exec timeout\nresult = sandbox.exec(\n    [\"python\", \"long_script.py\"],\n    timeout_seconds=300,  # 5 minute timeout\n).result()\n</code></pre> <p>This controls how long the client waits for a response. If exceeded, raises <code>SandboxTimeoutError</code>.</p>"},{"location":"guides/sandbox-configuration/#max_lifetime_seconds","title":"max_lifetime_seconds","text":"<p>Maximum sandbox lifetime (set in SandboxDefaults):</p> <pre><code>defaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    max_lifetime_seconds=3600,  # 1 hour max lifetime\n)\n\nwith Sandbox.run(defaults=defaults) as sandbox:\n    # Sandbox automatically terminates after 1 hour\n    pass\n</code></pre> <p>This is a server-side limit. The sandbox will be terminated when it reaches this age, regardless of activity.</p>"},{"location":"guides/sandbox-configuration/#complete-example","title":"Complete Example","text":"<pre><code>from aviato import Sandbox, SandboxDefaults\n\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    max_lifetime_seconds=1800,  # 30 minutes\n    tags=(\"production\", \"ml-pipeline\"),\n)\n\nwith Sandbox.run(\n    defaults=defaults,\n    resources={\n        \"cpu\": \"2000m\",\n        \"memory\": \"4Gi\",\n    },\n    mounted_files=[\n        {\n            \"path\": \"/app/config.yaml\",\n            \"content\": \"model: gpt-4\\nmax_tokens: 1000\",\n        },\n    ],\n    ports=[\n        {\"container_port\": 8000},\n    ],\n) as sandbox:\n    # Install dependencies\n    sandbox.exec([\"pip\", \"install\", \"fastapi\", \"uvicorn\"]).result()\n\n    # Run application\n    result = sandbox.exec(\n        [\"python\", \"-c\", \"print('Server started')\"],\n        timeout_seconds=60,\n    ).result()\n    print(result.stdout)\n</code></pre>"},{"location":"guides/sessions/","title":"Sessions Guide","text":"<p>This guide covers using <code>Session</code> to manage multiple sandboxes with shared configuration.</p>"},{"location":"guides/sessions/#what-is-a-session","title":"What is a Session?","text":"<p>A <code>Session</code> provides: - Shared default configuration for all sandboxes - Automatic cleanup when the session closes - A scope for the <code>@session.function()</code> decorator</p>"},{"location":"guides/sessions/#basic-usage","title":"Basic Usage","text":"<pre><code>import aviato\nfrom aviato import SandboxDefaults\n\n# Define shared configuration\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"project-alpha\",),\n)\n\n# Create a session with context manager\nwith aviato.Session(defaults=defaults) as session:\n    # Create sandboxes through the session\n    sb1 = session.sandbox()\n    sb2 = session.sandbox()\n\n    # Use the sandboxes\n    result1 = sb1.exec([\"echo\", \"sandbox 1\"]).result()\n    result2 = sb2.exec([\"echo\", \"sandbox 2\"]).result()\n\n# All sandboxes automatically stopped when exiting context\n</code></pre>"},{"location":"guides/sessions/#creating-sandboxes","title":"Creating Sandboxes","text":""},{"location":"guides/sessions/#sessionsandbox","title":"session.sandbox()","text":"<p>Creates and starts a sandbox with session defaults:</p> <pre><code>with aviato.Session(defaults=defaults) as session:\n    # sandbox() creates AND starts the sandbox\n    sandbox = session.sandbox()\n\n    # Sandbox is already started, ready to use\n    result = sandbox.exec([\"echo\", \"hello\"]).result()\n</code></pre>"},{"location":"guides/sessions/#overriding-defaults","title":"Overriding Defaults","text":"<p>Pass additional arguments to override session defaults:</p> <pre><code># Session with base configuration\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    tags=(\"my-project\",),\n)\n\nwith aviato.Session(defaults=defaults) as session:\n    # Override image and resources for this sandbox\n    gpu_sandbox = session.sandbox(\n        command=\"sleep\",\n        args=[\"infinity\"],\n        container_image=\"pytorch/pytorch:latest\",\n        resources={\"cpu\": \"4000m\", \"memory\": \"16Gi\", \"gpu\": \"1\"},\n        tags=[\"gpu-workload\"],  # Merged with session tags\n    )\n</code></pre> <p>See the Sandbox Configuration Guide for all available options.</p>"},{"location":"guides/sessions/#multiple-sandbox-management","title":"Multiple Sandbox Management","text":"<p>Sessions excel at managing sandbox pools:</p> <pre><code>with aviato.Session(defaults=defaults) as session:\n    # Create a pool of sandboxes\n    sandboxes = [\n        session.sandbox()\n        for _ in range(5)\n    ]\n\n    # Distribute work across the pool\n    processes = [\n        sb.exec([\"python\", \"-c\", f\"print({i})\"])\n        for i, sb in enumerate(sandboxes)\n    ]\n\n    # Collect results\n    results = [p.result() for p in processes]\n\n# All 5 sandboxes automatically cleaned up\n</code></pre>"},{"location":"guides/sessions/#session-lifecycle","title":"Session Lifecycle","text":""},{"location":"guides/sessions/#manual-close","title":"Manual Close","text":"<p>Close a session explicitly when not using context manager:</p> <pre><code>session = aviato.Session(defaults=defaults)\n\nsandbox = session.sandbox()\nresult = sandbox.exec([\"echo\", \"hello\"]).result()\n\n# Close the session (stops all sandboxes)\nsession.close().result()\n</code></pre>"},{"location":"guides/sessions/#what-close-does","title":"What close() Does","text":"<ol> <li>Stops all sandboxes created through the session</li> <li>Waits for cleanup to complete</li> <li>Returns <code>OperationRef[None]</code></li> </ol>"},{"location":"guides/sessions/#error-handling","title":"Error Handling","text":"<p>Sessions clean up even if exceptions occur:</p> <pre><code>with aviato.Session(defaults=defaults) as session:\n    sandbox = session.sandbox()\n    raise RuntimeError(\"Something went wrong!\")\n# Sandbox is still cleaned up\n</code></pre>"},{"location":"guides/sessions/#adopting-external-sandboxes","title":"Adopting External Sandboxes","text":"<p>Bring sandboxes created outside the session under session management:</p> <pre><code>import aviato\nfrom aviato import Sandbox\n\nwith aviato.Session(defaults=defaults) as session:\n    # Find existing sandboxes\n    existing = Sandbox.list(tags=[\"orphaned-work\"]).result()\n\n    # Adopt them for cleanup\n    for sandbox in existing:\n        session.adopt(sandbox)\n\n    # Now session.close() will clean them up too\n</code></pre>"},{"location":"guides/sessions/#session-properties","title":"Session Properties","text":"<pre><code>session = aviato.Session(defaults=defaults)\n\n# Number of sandboxes tracked\nprint(session.sandbox_count)  # 0\n\nsandbox = session.sandbox()\nprint(session.sandbox_count)  # 1\n</code></pre>"},{"location":"guides/sessions/#when-to-use-sessions","title":"When to Use Sessions","text":"Use Case Recommended Approach Single sandbox, simple task <code>Sandbox.run()</code> with context manager Multiple sandboxes, shared config Session Function decorator API Session required Pool of workers Session One-off command Direct <code>Sandbox.run()</code>"},{"location":"guides/sessions/#without-sessions","title":"Without Sessions","text":"<p>For single sandboxes, sessions aren't required:</p> <pre><code>from aviato import Sandbox\n\n# Direct usage without session\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"echo\", \"hello\"]).result()\n# Cleanup handled by context manager\n</code></pre>"},{"location":"guides/swebench/","title":"SWE-bench evaluation guide","text":"<p>Run SWE-bench evaluations in parallel using Aviato sandboxes.</p>"},{"location":"guides/swebench/#what-this-does","title":"What this does","text":"<p>SWE-bench tests whether language models can fix real bugs in real repositories. Each task: apply a patch, run the test suite, see if the fix works.</p> <p>You can run SWE-bench locally with Docker, but you're limited by your machine's resources. Aviato lets you run evaluations at scale on CoreWeave infrastructure. Spin up dozens or hundreds of sandboxes concurrently without managing any of it yourself. The script pulls pre-built images from Epoch AI's registry on GHCR, so there's no local Docker build step.</p>"},{"location":"guides/swebench/#setup","title":"Setup","text":"<p>Clone the repo and install dependencies:</p> <pre><code>git clone https://github.com/coreweave/aviato-client.git\ncd aviato-client\nuv sync\nuv pip install swebench datasets\n</code></pre> <p>For authentication, set one of these:</p> <ul> <li><code>AVIATO_API_KEY</code> environment variable (recommended)</li> <li><code>WANDB_API_KEY</code> environment variable</li> <li>W&amp;B credentials in <code>~/.netrc</code></li> </ul>"},{"location":"guides/swebench/#quick-start","title":"Quick start","text":"<p>Test your setup with a single instance. The <code>gold</code> option uses the known-correct fix from the dataset:</p> <pre><code>uv run python examples/swebench/run_evaluation.py \\\n    --predictions-path gold \\\n    --instance-ids astropy__astropy-12907 \\\n    --run-id test\n</code></pre> <p>This should pass.</p>"},{"location":"guides/swebench/#running-in-parallel","title":"Running in parallel","text":"<p>Run multiple instances at once:</p> <pre><code>uv run python examples/swebench/run_evaluation.py \\\n    --predictions-path gold \\\n    --instance-ids \\\n        astropy__astropy-12907 \\\n        django__django-11039 \\\n        django__django-11099 \\\n        django__django-11283 \\\n        matplotlib__matplotlib-23476 \\\n        scikit-learn__scikit-learn-13142 \\\n        sympy__sympy-13031 \\\n        sympy__sympy-13647 \\\n    --run-id parallel-test \\\n    --max-workers 8\n</code></pre> <p>This spins up 8 sandboxes and runs them concurrently. All should pass since gold patches are the correct fixes.</p>"},{"location":"guides/swebench/#evaluating-model-predictions","title":"Evaluating model predictions","text":"<p>To test custom model output:</p> <pre><code>uv run python examples/swebench/run_evaluation.py \\\n    --predictions-path predictions.json \\\n    --instance-ids django__django-11039 scikit-learn__scikit-learn-13142 \\\n    --run-id eval-run-1 \\\n    --max-workers 10\n</code></pre> <p>The predictions file maps instance IDs to patches:</p> <pre><code>[\n  {\n    \"instance_id\": \"django__django-11039\",\n    \"model_name_or_path\": \"gpt-4\",\n    \"model_patch\": \"diff --git a/...\"\n  }\n]\n</code></pre>"},{"location":"guides/swebench/#options","title":"Options","text":"Option Default Description <code>--predictions-path</code> Required Path to predictions JSON, or <code>gold</code> for gold patches <code>--instance-ids</code> Required Space-separated instance IDs <code>--run-id</code> Required Identifier for this run <code>--max-workers</code> 10 Max parallel sandboxes <code>--timeout</code> 1800 Per-instance timeout in seconds (30 min) <code>--output-dir</code> <code>logs/swebench</code> Where to write logs and reports <code>--dataset</code> <code>princeton-nlp/SWE-bench_Lite</code> HuggingFace dataset name <code>--force</code> false Re-run instances even if report.json exists"},{"location":"guides/swebench/#adjusting-resources","title":"Adjusting resources","text":"<p>Default is 2 CPUs and 4Gi memory per sandbox. Change this in <code>run_evaluation.py</code>:</p> <pre><code>defaults = SandboxDefaults(\n    tags=(f\"swebench-{run_id}\",),\n    resources={\"cpu\": \"4\", \"memory\": \"8Gi\"},\n)\n</code></pre>"},{"location":"guides/swebench/#how-it-works","title":"How it works","text":""},{"location":"guides/swebench/#container-images","title":"Container images","text":"<p>Epoch AI hosts pre-built images on GHCR. Each instance has its own image with the repo checked out at the right commit, dependencies installed, and test environment ready.</p> <p>Image format: <code>ghcr.io/epoch-research/swe-bench.eval.x86_64.{instance_id}:latest</code></p> <p>Sandboxes pull these directly from GHCR. No local builds needed.</p>"},{"location":"guides/swebench/#evaluation-flow","title":"Evaluation flow","text":"<p>Each instance goes through these steps:</p> Step Where What happens 1. Load dataset Local Fetch instance metadata from HuggingFace 2. Create sandbox Aviato Start sandbox with the instance's image 3. Write patch Aviato Write patch to <code>/tmp/patch.diff</code> 4. Apply patch Aviato Run <code>git apply</code> (falls back to <code>patch</code> if needed) 5. Run tests Aviato Execute <code>/root/eval.sh</code> 6. Grade results Local Parse output with <code>swebench.harness.grading</code> 7. Write report Local Save results to <code>logs/swebench/</code> 8. Cleanup Aviato Stop sandbox <p>Steps 2-5 and 8 run remotely. Steps 1, 6, and 7 run on your machine.</p>"},{"location":"guides/swebench/#parallel-execution","title":"Parallel execution","text":"<p>The script uses <code>ThreadPoolExecutor</code> to run instance workflows concurrently. Each thread drives one instance through its workflow. While one instance runs tests, another can be applying its patch, another grading locally, another starting up. The overlap is where the speed comes from.</p> <p>Results come back as workflows finish via <code>as_completed()</code>.</p>"},{"location":"guides/swebench/#cleanup","title":"Cleanup","text":"<p>The script uses an Aviato <code>Session</code> to track sandboxes. When the session exits (normal exit, exception, or Ctrl+C), all sandboxes get cleaned up.</p> <pre><code>with aviato.Session(defaults=defaults) as session:\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Sandboxes created here get cleaned up when the session exits\n</code></pre>"},{"location":"guides/swebench/#output","title":"Output","text":"<p>Results go to <code>{output-dir}/{run-id}/{model-name}/{instance-id}/</code>:</p> File Contents <code>report.json</code> Resolved status, sandbox ID, duration <code>test_output.txt</code> Full test output <code>patch.diff</code> The patch that was applied"},{"location":"guides/swebench/#report-format","title":"Report format","text":"<pre><code>{\n  \"instance_id\": {\n    \"resolved\": true,\n    \"tests_status\": {\n      \"PASSED\": [\"test_foo\", \"test_bar\"],\n      \"FAILED\": []\n    }\n  },\n  \"sandbox_id\": \"sb-abc123\",\n  \"duration_seconds\": 45.2\n}\n</code></pre> <p>The format is compatible with standard SWE-bench tooling.</p>"},{"location":"guides/swebench/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/swebench/#patch-application-fails","title":"Patch application fails","text":"<p>If you see <code>APPLY_PATCH_FAIL</code> in logs, the patch is probably malformed, targeting the wrong commit, or has whitespace issues. Try <code>git apply --check</code> locally to see what's wrong. Make sure the instance ID matches the prediction.</p>"},{"location":"guides/swebench/#tests-timeout","title":"Tests timeout","text":"<p>Some test suites genuinely take longer than 30 minutes. Model-generated code might also have infinite loops. Increase <code>--timeout</code> if needed, or check the test output to see where it's hanging.</p>"},{"location":"guides/swebench/#image-pull-errors","title":"Image pull errors","text":"<p>If the container fails to start with an image pull error, either the instance ID doesn't exist in Epoch AI's registry or there's a network issue reaching <code>ghcr.io</code>. Verify the instance ID is in the SWE-bench dataset.</p>"},{"location":"guides/swebench/#see-also","title":"See also","text":"<ul> <li>Example script: full source code</li> <li>Command execution guide: details on <code>exec()</code> patterns</li> <li>Cleanup patterns: managing sandbox lifecycle</li> </ul>"},{"location":"guides/sync-vs-async/","title":"Sync vs Async Patterns","text":"<p>The aviato SDK provides a unified sync/async hybrid API. All operations work with both patterns.</p>"},{"location":"guides/sync-vs-async/#quick-decision-guide","title":"Quick Decision Guide","text":"Use Case Pattern Reason Most operations Sync Simpler code, no asyncio boilerplate Parallel execution Sync Operations are non-blocking by design Jupyter notebooks Sync No nest_asyncio needed Async codebase Async Integrates with existing async code <p>Rule of thumb: Use sync patterns (<code>.result()</code>) for simplicity. All methods work with both sync and async patterns.</p>"},{"location":"guides/sync-vs-async/#sync-pattern-recommended-default","title":"Sync Pattern (Recommended Default)","text":"<p>All SDK operations work synchronously. Operations return immediately with lazy result objects:</p> <pre><code>from aviato import Sandbox\n\n# Sandbox.run() returns immediately\nsandbox = Sandbox.run()\n\n# .result() blocks for result\nresult = sandbox.exec([\"echo\", \"hello\"]).result()\nprint(result.stdout)  # \"hello\\n\"\n\n# Context manager for automatic cleanup\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"ls\"]).result()\n    print(result.stdout)\n</code></pre>"},{"location":"guides/sync-vs-async/#key-sync-methods","title":"Key Sync Methods","text":"<p>Most methods return <code>OperationRef</code>, a lazy object that implements a <code>.result()</code> blocking method to get the value while also being awaitable for async codebases.</p> <p>Sandbox <code>exec</code> calls return a <code>Process</code> object (which inherits from <code>OperationRef</code>) and uses <code>.result()</code> to block and return a <code>ProcessResult</code> instance with information about the command that was run. <code>Process</code> has additional functionality like output streaming, and is also awaitable for async codebases. </p> <ul> <li><code>Sandbox.run()</code> - Create and start sandbox (returns immediately)</li> <li><code>Sandbox.list()</code> - Query existing sandboxes (returns OperationRef)</li> <li><code>Sandbox.from_id()</code> - Attach to existing sandbox (returns OperationRef)</li> <li><code>Sandbox.delete()</code> - Delete sandbox by ID (returns OperationRef)</li> <li><code>sandbox.exec()</code> - Execute command (returns Process)</li> <li><code>sandbox.read_file()</code> - Read file contents (returns OperationRef)</li> <li><code>sandbox.write_file()</code> - Write file contents (returns OperationRef)</li> <li><code>sandbox.stop()</code> - Stop sandbox (returns OperationRef)</li> <li><code>sandbox.wait()</code> - Wait until RUNNING status</li> <li><code>sandbox.wait_until_complete()</code> - Wait until terminal status</li> <li><code>sandbox.get_status()</code> - Fetch fresh status</li> <li><code>session.list()</code> - Query sandboxes matching session tags (returns OperationRef)</li> <li><code>session.from_id()</code> - Attach and optionally adopt by ID (returns OperationRef)</li> </ul>"},{"location":"guides/sync-vs-async/#async-codebases","title":"Async Codebases","text":"<p><code>OperationRef</code> is awaitable, so the same methods work in async code without <code>.result()</code>:</p> <pre><code>import asyncio\nfrom aviato import Sandbox\n\nasync def main() -&gt; None:\n    # Sandbox.run() returns immediately\n    sandbox = Sandbox.run()\n\n    # await instead of .result()\n    result = await sandbox.exec([\"echo\", \"hello\"])\n    print(result.stdout)  # \"hello\\n\"\n\n    # Async context manager for automatic cleanup\n    async with Sandbox.run() as sandbox:\n        result = await sandbox.exec([\"ls\"])\n        print(result.stdout)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/sync-vs-async/#parallel-execution","title":"Parallel Execution","text":"<p>The sync API supports parallel execution because operations are non-blocking by design. Methods like <code>exec()</code>, <code>read_file()</code>, and <code>write_file()</code> return immediately - you only block when you call <code>.result()</code>.</p> <pre><code>from aviato import Sandbox\n\n# Start multiple sandboxes (each returns immediately)\nsandboxes = [Sandbox.run() for _ in range(3)]\n\n# Fire off commands on each sandbox (non-blocking)\nprocesses = [sb.exec([\"echo\", f\"sandbox-{i}\"]) for i, sb in enumerate(sandboxes)]\n\n# Now block for all results\nresults = [p.result() for p in processes]\nfor r in results:\n    print(r.stdout)\n\n# Clean up\nfor sb in sandboxes:\n    sb.stop()\n</code></pre> <p>This pattern executes commands in parallel without any async code. The key insight: non-blocking != async. The sync API is non-blocking, you just use <code>.result()</code> to block when you need results.</p>"},{"location":"guides/sync-vs-async/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>The sync API works in Jupyter without <code>nest_asyncio</code>:</p> <pre><code># Cell 1 - Create sandbox\nfrom aviato import Sandbox\nsandbox = Sandbox.run()\nsandbox.wait()  # Wait until RUNNING\n\n# Cell 2 - Execute commands\nresult = sandbox.exec([\"python\", \"-c\", \"print(1+1)\"]).result()\nprint(result.stdout)\n\n# Cell 3 - Discovery\nsandboxes = Sandbox.list(tags=[\"notebook\"]).result()\nprint(f\"Found {len(sandboxes)} sandboxes\")\n\n# Cell 4 - Cleanup\nsandbox.stop().result()\n</code></pre> <p>For async operations in Jupyter, use <code>await</code> directly (Jupyter has a built-in event loop):</p> <pre><code># Works in Jupyter without asyncio.run()\nsandboxes = await Sandbox.list(tags=[\"notebook\"])\n</code></pre>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide covers common issues and their solutions when working with the Aviato SDK.</p>"},{"location":"guides/troubleshooting/#authentication-issues","title":"Authentication Issues","text":"<p>Symptom: <code>AviatoAuthenticationError</code> or <code>WandbAuthError</code> raised on sandbox operations.</p> <p>The SDK resolves authentication in this order:</p> <ol> <li><code>AVIATO_API_KEY</code> env var (takes priority)</li> <li><code>WANDB_API_KEY</code> + <code>WANDB_ENTITY_NAME</code> env vars</li> <li><code>~/.netrc</code> (api.wandb.ai) + <code>WANDB_ENTITY_NAME</code></li> </ol> <p>You need one of these configured. Check which method you're using:</p> <pre><code># Option 1: Aviato API key\necho $AVIATO_API_KEY\n\n# Option 2: W&amp;B credentials\necho $WANDB_API_KEY\necho $WANDB_ENTITY_NAME\n</code></pre>"},{"location":"guides/troubleshooting/#common-issues","title":"Common Issues","text":"Issue Solution No credentials configured Set <code>AVIATO_API_KEY</code> or W&amp;B credentials Invalid or expired API key Contact your administrator for a new key W&amp;B API key set but entity missing Set <code>WANDB_ENTITY_NAME</code> to your W&amp;B entity/team Using netrc but entity missing Set <code>WANDB_ENTITY_NAME</code> - it's always required for W&amp;B Netrc parse errors Check <code>~/.netrc</code> file syntax and permissions"},{"location":"guides/troubleshooting/#command-execution-issues","title":"Command Execution Issues","text":""},{"location":"guides/troubleshooting/#timeout-tuning","title":"Timeout Tuning","text":"<p>The SDK has two types of timeouts:</p> Timeout Scope Default Where Set <code>timeout_seconds</code> Per-exec None <code>exec()</code> parameter <code>max_lifetime_seconds</code> Sandbox lifetime Backend-controlled <code>SandboxDefaults</code> <p>Client-side timeout (<code>timeout_seconds</code>):</p> <pre><code>from aviato import SandboxTimeoutError\n\ntry:\n    result = sandbox.exec(\n        [\"python\", \"slow_script.py\"],\n        timeout_seconds=60.0,  # 60 second timeout\n    ).result()\nexcept SandboxTimeoutError:\n    print(\"Command timed out\")\n</code></pre> <p>Server-side lifetime (<code>max_lifetime_seconds</code>):</p> <pre><code>from aviato import SandboxDefaults\n\ndefaults = SandboxDefaults(\n    container_image=\"python:3.11\",\n    max_lifetime_seconds=3600,  # Sandbox terminates after 1 hour\n)\n</code></pre>"},{"location":"guides/troubleshooting/#long-running-commands","title":"Long-Running Commands","text":"<p>Issue: Command takes longer than expected.</p> <p>Solutions:</p> <ol> <li>Set appropriate timeout:</li> </ol> <pre><code>process = sandbox.exec(\n    [\"pip\", \"install\", \"tensorflow\"],\n    timeout_seconds=300.0,  # 5 minutes for large packages\n)\nresult = process.result()\n</code></pre> <ol> <li>Use streaming to monitor progress:</li> </ol> <pre><code>process = sandbox.exec([\"pip\", \"install\", \"tensorflow\"])\n\nfor line in process.stdout:\n    print(line, end=\"\")\n\nresult = process.result()\n</code></pre>"},{"location":"guides/troubleshooting/#exit-code-interpretation","title":"Exit Code Interpretation","text":"<p>Issue: Understanding command failures.</p> <p>Exit codes follow Unix conventions:</p> Code Meaning 0 Success 1 General error 2 Misuse of command 126 Command not executable 127 Command not found 128+N Killed by signal N <p>Use <code>check=True</code> to raise on non-zero exit:</p> <pre><code>from aviato import SandboxExecutionError\n\ntry:\n    result = sandbox.exec(\n        [\"python\", \"-c\", \"import nonexistent\"],\n        check=True,\n    ).result()\nexcept SandboxExecutionError as e:\n    print(f\"Exit code: {e.exec_result.returncode}\")\n    print(f\"stderr: {e.exec_result.stderr}\")\n</code></pre>"},{"location":"guides/troubleshooting/#streaming-output-issues","title":"Streaming Output Issues","text":""},{"location":"guides/troubleshooting/#line-buffering-behavior","title":"Line Buffering Behavior","text":"<p>Issue: Output appears delayed or all at once when streaming.</p> <p>Python buffers stdout when not connected to a TTY. Force unbuffered output:</p> <pre><code># Option 1: Use -u flag\nprocess = sandbox.exec([\"python\", \"-u\", \"script.py\"])\nfor line in process.stdout:\n    print(line, end=\"\")\nresult = process.result()\n\n# Option 2: Set PYTHONUNBUFFERED\nprocess = sandbox.exec([\"sh\", \"-c\", \"PYTHONUNBUFFERED=1 python script.py\"])\nfor line in process.stdout:\n    print(line, end=\"\")\nresult = process.result()\n</code></pre> <p>For your own scripts, flush explicitly:</p> <pre><code>print(\"Progress...\", flush=True)\n</code></pre>"},{"location":"guides/troubleshooting/#cleanup-problems","title":"Cleanup Problems","text":""},{"location":"guides/troubleshooting/#orphaned-sandboxes","title":"Orphaned Sandboxes","text":"<p>Issue: Sandboxes remain running after script exits or crashes.</p> <p>Prevention: Use context managers:</p> <pre><code># Recommended: automatic cleanup\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"echo\", \"hello\"]).result()\n# Sandbox stopped automatically\n</code></pre> <p>See Cleanup Patterns - Orphan Management for how to find and clean up orphaned sandboxes by tag.</p>"},{"location":"guides/troubleshooting/#common-error-messages","title":"Common Error Messages","text":"Error Cause Solution <code>AviatoAuthenticationError</code> Missing or invalid credentials Check <code>AVIATO_API_KEY</code> is set <code>WandbAuthError: WANDB_ENTITY_NAME is not set</code> W&amp;B API key found but entity missing Set <code>WANDB_ENTITY_NAME</code> env var <code>SandboxNotRunningError</code> Operation on stopped sandbox Check <code>sandbox.status</code> before operations <code>SandboxTimeoutError</code> Operation exceeded timeout Increase <code>timeout_seconds</code> or optimize command <code>SandboxTerminatedError</code> Sandbox killed externally Check <code>max_lifetime_seconds</code> or external termination <code>SandboxFailedError</code> Sandbox failed to start Check container image and resources <code>SandboxNotFoundError</code> Sandbox deleted or never existed Verify sandbox ID is correct <code>SandboxExecutionError</code> Command returned non-zero (with <code>check=True</code>) Check <code>e.exec_result.stderr</code> for details <code>SandboxFileError</code> File operation failed Check file path and permissions <code>FunctionSerializationError</code> Can't serialize function args Use JSON-serializable types or <code>Serialization.PICKLE</code> <code>AsyncFunctionError</code> Async function passed to <code>@session.function()</code> Use sync functions only"},{"location":"tutorial/01-first-sandbox/","title":"1. Your First Sandbox","text":""},{"location":"tutorial/01-first-sandbox/#setup","title":"Setup","text":"<p>Clone and install the SDK locally:</p> <pre><code>git clone https://github.com/coreweave/aviato-client.git\ncd aviato-client\nuv sync\n</code></pre> <p>Set your credentials:</p> <pre><code># Option 1: Aviato API key\nexport AVIATO_API_KEY=\"your-api-key\"\n\n# Option 2: W&amp;B credentials\nexport WANDB_API_KEY=\"your-wandb-key\"\nexport WANDB_ENTITY_NAME=\"your-entity\"\n</code></pre>"},{"location":"tutorial/01-first-sandbox/#run-your-first-sandbox","title":"Run Your First Sandbox","text":"<pre><code>from aviato import Sandbox\n\nwith Sandbox.run() as sandbox:\n    result = sandbox.exec([\"echo\", \"Hello!\"]).result()\n    print(result.stdout)\n</code></pre> <p>What's happening:</p> <ul> <li><code>Sandbox.run()</code> creates a sandbox and returns it inside a context manager</li> <li><code>exec()</code> runs a command and returns a <code>Process</code> object</li> <li><code>.result()</code> waits for the command to complete and returns the output</li> </ul>"},{"location":"tutorial/02-configuration/","title":"2. Configuring Sandboxes","text":"<p>Pass configuration options directly to <code>Sandbox.run()</code>:</p> <pre><code>from aviato import Sandbox\n\nwith Sandbox.run(\n    container_image=\"python:3.11\",\n    max_lifetime_seconds=300,\n    tags=[\"my-app\"],\n) as sandbox:\n    sandbox.exec([\"python\", \"--version\"]).result()\n</code></pre>"},{"location":"tutorial/02-configuration/#resources","title":"Resources","text":"<p>Request CPU and memory:</p> <pre><code>with Sandbox.run(resources={\"cpu\": \"1\", \"memory\": \"1Gi\"}) as sandbox:\n    sandbox.exec([\"python\", \"compute.py\"]).result()\n</code></pre> <p>Uses Kubernetes resource syntax:</p> Resource Format Examples CPU Cores or millicores <code>\"1\"</code>, <code>\"2\"</code>, <code>\"500m\"</code> (0.5 CPU) Memory Bytes with unit suffix <code>\"512Mi\"</code>, <code>\"1Gi\"</code>, <code>\"2Gi\"</code>"},{"location":"tutorial/02-configuration/#mounted-files","title":"Mounted Files","text":"<p>Pre-populate files at sandbox startup:</p> <pre><code>with Sandbox.run(\n    mounted_files=[\n        {\"path\": \"/app/config.json\", \"content\": '{\"debug\": true}'},\n        {\"path\": \"/app/script.py\", \"content\": \"print('hello')\"},\n    ]\n) as sandbox:\n    sandbox.exec([\"python\", \"/app/script.py\"]).result()\n</code></pre> <p>Mounted files are read-only. Use <code>write_file()</code> for files that need modification.</p> <p>For reusable configuration across multiple sandboxes, use <code>SandboxDefaults</code>. See the Sandbox Configuration Guide for all available options including GPU resources, ports, and services.</p>"},{"location":"tutorial/03-running-commands/","title":"3. Running Commands","text":"<pre><code># Basic command\nsandbox.exec([\"echo\", \"Hello\"]).result()\n\n# Raise SandboxExecutionError if command returns non-zero exit code\nsandbox.exec([\"ls\", \"/nonexistent\"], check=True).result()\n\n# Raise SandboxTimeoutError if command exceeds timeout\nsandbox.exec([\"sleep\", \"60\"], timeout_seconds=5).result()\n\n# Run in a specific directory\nsandbox.exec([\"ls\"], cwd=\"/app\").result()\n</code></pre>"},{"location":"tutorial/03-running-commands/#parallel-execution","title":"Parallel Execution","text":"<pre><code>import aviato\n\n# exec() returns a Process immediately without waiting for the command to finish\np1 = sandbox.exec([\"sleep\", \"1\"])\np2 = sandbox.exec([\"sleep\", \"1\"])\np3 = sandbox.exec([\"sleep\", \"1\"])\n\n# Call .result() when you need the output - ~1 second total, not 3\naviato.result([p1, p2, p3])\n</code></pre> <p>For streaming, error handling, and process control, see the Execution Guide.</p>"},{"location":"tutorial/04-streaming/","title":"4. Streaming Output","text":"<p>Get output as it happens instead of waiting for completion:</p> <pre><code>process = sandbox.exec([\"bash\", \"-c\", \"for i in 1 2 3; do echo $i; sleep 1; done\"])\n\nfor line in process.stdout:\n    print(line, end=\"\")\n\nresult = process.result()\n</code></pre> <p>Note: To stream output in real-time, iterate over <code>process.stdout</code> before calling <code>.result()</code>. If you only need the final output, you can skip iteration and access it via <code>result.stdout</code> after calling <code>.result()</code>.</p> <p>For more streaming patterns, see the Execution Guide.</p>"},{"location":"tutorial/05-files/","title":"5. Reading &amp; Writing Files","text":"<pre><code># Write (content must be bytes)\nsandbox.write_file(\"/tmp/hello.txt\", b\"Hello!\").result()\n\n# Read (returns bytes)\ncontent = sandbox.read_file(\"/tmp/hello.txt\").result()\nprint(content.decode())\n</code></pre> <p>Like <code>exec()</code>, file operations return an <code>OperationRef</code> immediately without waiting for completion. Call <code>.result()</code> when you need the data. This lets you start multiple operations and wait for them together:</p> <pre><code>refs = [sandbox.write_file(f\"/tmp/{i}.txt\", b\"data\") for i in range(5)]\nfor ref in refs:\n    ref.result()\n</code></pre> <p>For parallel upload/download patterns and S3 mounts, see the File Operations Guide.</p>"},{"location":"tutorial/06-multiple-sandboxes/","title":"6. Managing Multiple Sandboxes","text":"<p>Sessions manage multiple sandboxes with shared defaults and automatic cleanup:</p> <pre><code>from aviato import Sandbox, SandboxDefaults\n\ndefaults = SandboxDefaults(container_image=\"python:3.11\", tags=(\"my-app\",))\n\nwith Sandbox.session(defaults) as session:\n    sb1 = session.sandbox()\n    sb2 = session.sandbox()\n\n    p1 = sb1.exec([\"echo\", \"one\"])\n    p2 = sb2.exec([\"echo\", \"two\"])\n\n    print(p1.result().stdout, p2.result().stdout)\n# All sandboxes cleaned up\n</code></pre> <p>For sandbox pools, adoption patterns, and lifecycle management, see the Sessions Guide.</p>"},{"location":"tutorial/07-remote-functions/","title":"7. Remote Function Execution","text":"<p>Run Python functions in sandboxes without writing command strings:</p> <pre><code>from aviato import Sandbox, SandboxDefaults\n\nwith Sandbox.session(SandboxDefaults()) as session:\n    @session.function()\n    def add(x: int, y: int) -&gt; int:\n        return x + y\n\n    result = add.remote(2, 3).result()  # 5\n</code></pre> <p>Parallel execution with <code>.map()</code>:</p> <pre><code>@session.function()\ndef square(x: int) -&gt; int:\n    return x * x\n\nrefs = square.map([(1,), (2,), (3,)])\nresults = [r.result() for r in refs]  # [1, 4, 9]\n</code></pre> <p>For serialization modes, <code>.local()</code> testing, and more, see the Remote Functions Guide.</p>"},{"location":"tutorial/08-cleanup/","title":"8. Cleanup","text":"<p>Context managers handle cleanup automatically:</p> <pre><code>with Sandbox.run() as sandbox:\n    sandbox.exec([\"echo\", \"hello\"]).result()\n# Stopped automatically\n</code></pre> <p>For sandboxes created without a context manager, call <code>.stop()</code> explicitly:</p> <pre><code>sandbox = Sandbox.run()\nsandbox.exec([\"echo\", \"hello\"]).result()\nsandbox.stop().result()\n</code></pre> <p>The SDK also registers global cleanup handlers for <code>atexit</code> and signals (Ctrl+C, SIGTERM), so sandboxes are stopped even on unexpected exits.</p> <p>For batch cleanup, tagging strategies, and orphan recovery, see the Cleanup Patterns Guide.</p>"}]}